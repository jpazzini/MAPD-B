{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91590d5d",
   "metadata": {},
   "source": [
    "# Lecture 2: Spark Dataframes\n",
    "\n",
    "Spark SQL is the Spark module dedicated to store and process structured datasets.\n",
    "\n",
    "As we have discussed already at the beginning at this course, structured datasets are very common in both scientific and non-scientific environments.\n",
    "\n",
    "Structured data is typically represented using abstractions such as Pandas Dataframes or Relational Databases tables. \n",
    "\n",
    "In Spark, structured datasets are referred to as Spark Dataframes.\n",
    "A Spark Dataframes is still built on top of the RDD low-level API, offering the same underlying logic concerning storing and partitioning data across multiple nodes, but also provide a richer set of functionalities and optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38914c8b",
   "metadata": {},
   "source": [
    "## Create and Start a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build a SparkSession \n",
    "#   connect to the master node on the port where the master node is listening (7077)\n",
    "#   declare the app name \n",
    "#   configure the executor memory to 512 MB\n",
    "#   either *connect* or *create* a new Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"My dataframe spark application\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beebfa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa4a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# print its status\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0dbef",
   "metadata": {},
   "source": [
    "## Creating a Spark DataFrame\n",
    "\n",
    "A Spark DataFrame is a **distributed** collection of data grouped into named columns.\n",
    "\n",
    "A Spark DataFrame is already very similar to the Pandas DataFrame API, and it can be created in a multitude of possible ways:\n",
    "\n",
    "- creating and appending a list of Spark Rows (Spark objects for records) with an *implicit* schema definition\n",
    "- creating and appending a list of Python tuples with a *explicit* schema definition\n",
    "- importing a Pandas DataFrame\n",
    "- from the results of a SQL query over a database or another DataFrame\n",
    "- from a collection of input files privided with a schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec2328",
   "metadata": {},
   "source": [
    "### Create a spark dataframe from a list of Rows\n",
    "\n",
    "The Spark `Row` is the native Spark object that hosts the data of a record in a Spark Dataframe.\n",
    "It's equivalent to a Python tuple or a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f820ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark dataframe from a list of Rows\n",
    "\n",
    "# import the Row class from the pyspark.sql module\n",
    "from pyspark.sql import Row\n",
    "from datetime import date, datetime\n",
    "\n",
    "# create a list of Row objects\n",
    "#   each Row object contains a list of values\n",
    "#   the list of values can be of different types\n",
    "#   the list of values must be in the same order as the list of columns\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "\n",
    "# print the spark dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73417a",
   "metadata": {},
   "source": [
    "### Create a spark dataframe from a list of tuples\n",
    "\n",
    "PySpark allows to pass as records a list of Python tuples.\n",
    "However, to allow Spark to interpret correctly the tuple fields, an **explicit schema definition** is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark dataframe from a list of Python tuples\n",
    "\n",
    "# create a list of Python tuples\n",
    "#   when creating the spark dataframe we must also \n",
    "#   pass the definition of the data types (i.e. the schema)\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "\n",
    "# print the spark dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7872fad9",
   "metadata": {},
   "source": [
    "### Create a spark dataframe from a Pandas dataframe\n",
    "\n",
    "PySpark can create a dataframe starting from a Pandas dataframe.\n",
    "\n",
    "It should be noted that Pandas dataframe is meant to be stored in memory of a local machine, while Spark is designed for hosting large datasets on storage/memory of multiple nodes.\n",
    "\n",
    "Moving data that can be hosted on a local machine using Pandas to a remote cluster using Spark is tipically not a very common usecase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6383baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# create an equivalent pandas dataframe\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "\n",
    "# create the spark dataframe from the pandas one\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# print the spark dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a4973",
   "metadata": {},
   "source": [
    "As in the case of the creation of an RDD, the mere creation of a Spark DataFrame does not imply any work is submitted to the executors.\n",
    "\n",
    "Whenever we ask Spark to retrieve something from the dataset we instead trigger the execution of the jobs (as usual, subdivided in stages and tasks), starting from loading the dataset into memory/storage of the remote cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ef1cf",
   "metadata": {},
   "source": [
    "To use a Spark DataFrame we have similar interfaces as in Pandas:\n",
    "- `DataFrame.show()` to visualize its content (i.e. its state/instance)\n",
    "- `DataFrame.printSchema()` to visualize the schema of the structured dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 rows (if available) of the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the schema of the DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c8db3",
   "metadata": {},
   "source": [
    "## Loading structured data\n",
    "\n",
    "By default Spark can create a DataFrame from data stored in many formats such as `csv`, `json`, `avro`, `parquet`, and many other listed [here](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html). \n",
    "\n",
    "If your dataset is stored in a format that Spark DataFrame cannot understand and interpret natively, it is always possible to first create an RDD as in the previous lecture, and later convert the RDD into a DataFrame with the `toDF()` functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba4a26",
   "metadata": {},
   "source": [
    "Here we create some mock-up structured data in the form of a simple set of records with 2 features: `feature1` and `feature2`.\n",
    "\n",
    "We can parallelize the creation of the `Rows` in Spark as if we were loading and \"unpacking\" or parsing data from a set of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy for random\n",
    "import numpy as np\n",
    "\n",
    "# mock-up function to load data from a file\n",
    "def read_custom_data(file_name):\n",
    "    # this is a placeholder code for some operation\n",
    "    # you might have to perform when reading the files\n",
    "    # for example, you might have to read the file line by line\n",
    "    # and parse the lines into numerical values \n",
    "    custom_data = []\n",
    "    # create 5 random events\n",
    "    for _ in range(5):\n",
    "        # each event is a pair of random numbers\n",
    "        event = {\n",
    "            'feature1': np.random.random(),\n",
    "            'feature2': np.random.random()\n",
    "        }\n",
    "        # append each event to the list as a Spark Row object\n",
    "        custom_data.append(Row(**event))\n",
    "    return custom_data\n",
    "    \n",
    "# fake list of files to read\n",
    "file_list = ['file1', 'file2']\n",
    "\n",
    "# create an RDD parallelizing the read function on the file list\n",
    "rdd = sc.parallelize(file_list)\\\n",
    "        .flatMap(read_custom_data)\n",
    "\n",
    "# the RDD contains a list of Row objects, which can be converted \n",
    "# to a DataFrame by using the toDF() function\n",
    "df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7de1e0",
   "metadata": {},
   "source": [
    "The `df` object is now a Spark DataFrame containing a list of Rows.\n",
    "\n",
    "However, Spark will always remember the DataFrame is actually a RDD, and we can always operate and retrieve the underlying RDD object from the DataFrame API, simply by using `collect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7076a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the whole set of values in the RDD\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the same operation on the dataframe\n",
    "df.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d47b452",
   "metadata": {},
   "source": [
    "Issuing a `show()` or other high-level API functionalities designed for the DataFrame will instead \"wrap\" aroud the RDD to present the output with the proper DataFrame schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call a DataFrame show() method to display the first 5 rows of the DataFrame.\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffbf4c",
   "metadata": {},
   "source": [
    "## The `pyspark.pandas` API\n",
    "\n",
    "Quite recently PySpark introduced a dedicated `Pandas` API on Spark, geared mainly for new users to make it even easier to move to and from the Pandas DataFrame and the PySpark DataFrame equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066bf1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pandas api from pyspark\n",
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c4f88",
   "metadata": {},
   "source": [
    "With the `pyspark.pandas` API the DataFrame creation is now streamlined and almost identical to plain Pandas.\n",
    "\n",
    "Let's create a simple Pandas DataFrame and the equivalent PySpark one using the `pyspark.pandas` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad249e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy pandas dataframe\n",
    "pandas_df = pd.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045129e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the pandas dataframe\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the equivalent pyspark pandas dataframe\n",
    "pyspark_pandas_df = ps.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702518fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the pyspark pandas dataframe\n",
    "pyspark_pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c716ba",
   "metadata": {},
   "source": [
    "A good fraction of the common `Pandas` functions will still work on `pyspark.pandas`, so most probably a good fraction of the pure-Pandas code you have might as well work in Spark with little effort, and thus be parallelized and sped-up with little-to-no effort. \n",
    "\n",
    "However, not _all_ Pandas functionalities are implemented in PySpark.\n",
    "For this reason we will mostly use the PySpark DataFrame in this notebook to continue with the more \"traditional\" and robust approach.  \n",
    "\n",
    "Nonetheless, you are suggested to take a look at the documentation on the Pandas API on Spark at the [link](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html), and to the [best practices](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html) and [supported pandas API](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/supported_pandas_api.html) if you want to use the PySpark-Pandas API more in your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48da719",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "We also have to keep in mind that ***Pandas and PySpark are very different \"under the hood\"***.\n",
    "Where **Pandas** is **single-threaded** and will host the DataFrame data in **local memory**, **PySpark** is designed to host data into **partitions** using the **executors' memory (or storage)** and process it in a **distributed** way.\n",
    "\n",
    "This imply that many operations that are ideally very simple might be either impossible or simply very inefficient in PySpark.\n",
    "\n",
    "_For instance... how do you think the ordering of the entries in the table is handled when your dataframe is split into possibly hundreds of partitions?\n",
    "What if you want a dataframe to be sorted?_\n",
    "\n",
    "The data in a Spark dataframe does not preserve the natural order by default...\n",
    "We can force Spark to rearrange data in its \"natural\" (e.g. from an index) order but this causes a performance overhead with an additional stage of internal data sorting (a wide-depenency transformation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2f9e1",
   "metadata": {},
   "source": [
    "## DataFrame analysis Example\n",
    "\n",
    "Let's run an end-to-end example of a simple data analysis performed using the PySpark DataFrame API.\n",
    "\n",
    "For this example we'll use a subsample of data produced by a particle physics experiment at CERN, and will attempt to reconstruct the mass spectrum of a particle from its decay product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002aabf8",
   "metadata": {},
   "source": [
    "### The dimuon mass spectrum\n",
    "\n",
    "Several particles decay in a pair of opposite charged leptons (electrons, muons and taus).\n",
    "\n",
    "The dimuon spectrum, computed by calculating the invariant mass of muon pairs with opposite charge, features the presence of a number of narrow resonances, corresponding to the mass of the parent particle: from the Î· meson at about 548 MeV  up to the Z boson at about 91 GeV.\n",
    "\n",
    "Rare processes are also associated to this very same final state, such as the Bs dimuon decay (first observed in 2012 at CMS and LHCb), and the elusive Higgs dimuon decay (for which there is statistical evidence, but not yet an observation); new yet undiscovered particles might also show up as new resonances in the dimuon spectrum as the statistics and accelerator energy is increased.\n",
    "\n",
    "![Event Display](imgs/lecture2/event_display.png)\n",
    "\n",
    "The dataset used in this exercise is taken from the CERN Open Data portal (https://opendata.cern.ch/) and represents only a tiny portion of the whole data collected by the CMS collaboration at the Large Hadron Collider in 2010.\n",
    "\n",
    "This dataset is comprised of only the fraction of events (i.e. particle collisions) retained by online selections (trigger) which identify collisions where muons have been produced, thus storing only about 10 events out of the about 40 millions of collisions produced every second at the LHC.\n",
    "\n",
    "The whole dataset collected by the CMS collaboration since the start of LHC operations in 2010 is comprised of roughly 100 PBs of data, and even more if taking into account the simulations.\n",
    "\n",
    "A small subset of the events have been extracted and preprocessed by Matteo, our T.A., who have stored the resulting records as `json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd1e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all available files\n",
    "!ls -l ../datasets/lecture2/dimuon/*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0d71b",
   "metadata": {},
   "source": [
    "#### Step 0 - Load and inspect the data\n",
    "\n",
    "Load the `json` files directly in a Spark DataFrame using the built-in data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset found in  the datasets/lecture2/dimuon folder\n",
    "#    spark.read --> read from a source of data\n",
    "#    option     --> specify the options for the data source\n",
    "#    format     --> specify the format of the data source\n",
    "#    load       --> specify the path of the inpute data\n",
    "df = spark.read \\\n",
    "    .option(\"inferTimestamp\",\"false\") \\\n",
    "    .option(\"prefersDecimal\",\"false\") \\\n",
    "    .format('json') \\\n",
    "    .load('../datasets/lecture2/dimuon/*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf996c8",
   "metadata": {},
   "source": [
    "Once again, the data at this stage is not loaded in a DataFrame just yet.\n",
    "Reading and loading files from source to an RDD (the low-level representation of the DataFrame) is a transformation, and therefore is _lazy_.\n",
    "\n",
    "Let's inspect the schema of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the schema of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215cea4b",
   "metadata": {},
   "source": [
    "The schema is quite interesting, as the data is not entirely _flat_.\n",
    "\n",
    "We have two scalar objects, `nMuon` and `sample`, respectively an integer and a string, per record.\n",
    "However, every record also contains a data structure named `Muons`, an array where each element contains the five numerical fields `E`, `charge`, `px`, `py`, `pz`.\n",
    "\n",
    "We can inspect a few elements by showing their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e41b580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the first 5 rows of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6a7ec",
   "metadata": {},
   "source": [
    "In this example, `sample` is a simplified element representing whether the records are from _real data_ or from a _simulated sample_ generated with a MonteCarlo technique.\n",
    "\n",
    "We can evaluate how populated are the two samples with an aggregation operation using `groupBy`.\n",
    "\n",
    "`groupBy` acts in a similar way with respect to its `SQL` and `Pandas` counterparts, by creating groups from the DataFrame data using the specified columns, onto which we can apply an aggregation function.\n",
    "\n",
    "For a complete list of aggregation functions you can refer to the (documentation)[https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.GroupedData.html#pyspark.sql.GroupedData]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of records in each sample group\n",
    "# \n",
    "#   1. apply a .groupBy() transformation on the dataframe, grouping on the sample\n",
    "#   2. use a .count() aggregator \n",
    "#   3. show the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8089ec",
   "metadata": {},
   "source": [
    "All transformations on the DataFrame will start from loading the files from storage.\n",
    "This is particularly inefficient if we will need to run multiple sets of operations (transformations+action) on the same dataset.\n",
    "\n",
    "We can however place the DataFrame in memory as for the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473375c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the dataframe to memory to avoid recomputing \n",
    "# everything from scratch at each iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d304c03e",
   "metadata": {},
   "source": [
    "We have already mentioned this but it's important: `persist` is not an in-place operation.\n",
    "\n",
    "With:\n",
    "```python\n",
    "df = df.persist()\n",
    "````\n",
    "Spark will create an entire copy of the DataFrame in the executors' memory.\n",
    "Every further operation using `df` will be run using the dataset stored in memory.\n",
    "\n",
    "With:\n",
    "```python\n",
    "df.persist()\n",
    "````\n",
    "Spark will still create an entire copy of the DataFrame in the executors' memory.\n",
    "However, every further operation using `df` will be run by loading the dataset every single time starting from files.\n",
    "\n",
    "`persist` is also considered a _transformation_ thus we need to trigger it in order to have effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's trigger persist with a count operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aebc1e6",
   "metadata": {},
   "source": [
    "#### Step 1 - First feature aggregation and plotting\n",
    "\n",
    "Let's finally start investigating the dataset by extracting some meaningful information.\n",
    "\n",
    "A first interesting information is to produce the distribution (an histogram) of the number of muons we have per event (thus per record) both in the `data` and in the `mc` samples, to compare them and see whether they do match.\n",
    "\n",
    "We can start by grouping the records by the number of muons per record and sample and count over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cdbaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the records by the sample \n",
    "# and the number of muons\n",
    "#\n",
    "# 1. get the number of muons per sample\n",
    "# 2. count them\n",
    "# 3. sort them based on the number of muons\n",
    "# 4. collect the results\n",
    "num_muons_dist = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f88c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 entries\n",
    "num_muons_dist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228a7a7",
   "metadata": {},
   "source": [
    "We have retrieved a pre-computed quantity from the cluster, thus having Spark do the \"heavy-lifting\" in a distributed way.\n",
    "The result is still a collection of `Row` object, in fact an RDD... not really the plot we would like, nor a simple Python object we can use to plot.\n",
    "We could retrieve directly a plot from Spark if we wanted, having it compute all features required to populate a graph, but that would require a lot of code.\n",
    "\n",
    "Fortunately though there is a trick we can use: it is possible to convert a Spark dataframe back into a Pandas Dataframe with `toPandas()`. \n",
    "\n",
    "Again, as it happens for `collect()`, the entire resulting dataset is fetched into the master and it may not fit in the memory, so do not use `toPandas()` very lightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24545648",
   "metadata": {},
   "source": [
    "[**Side Note on Apache Arrow**]\n",
    "\n",
    "_Apache Arrow comes into play in this contex: it is an in-memory columnar data format that is used in spark to efficiently transfer data between the JVM and Python processes. When it is not enabled Spark comunicates with python processes by serializing and deserializing one element of the time. With Arrow this operation is \"vectorized\", i.e. one column at the time is transfered. This is possible because both spark and pandas included Arrow representation._\n",
    "\n",
    "_[Here](https://xuechendi.github.io/2019/04/16/Apache-Arrow) you can find a nice blog post explaining how it works._\n",
    "\n",
    "_In this way, operations between PySpark and Pandas are more efficient and we can try to get the best from both worlds._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ee0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# do not collect the results as a plain RDD \n",
    "# but transform it into a pandas dataframe\n",
    "# using the .toPandas() function\n",
    "num_muons_dist = \n",
    "    \"\"\" ... \"\"\"\n",
    "    .toPandas()\n",
    "\n",
    "# print the first few rows of the pandas dataframe\n",
    "num_muons_dist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8f6a4",
   "metadata": {},
   "source": [
    "This way we have \"closed the loop\".\n",
    "\n",
    "- We have loaded data from a large number of files (possibly stored on a remote or distributed File System)\n",
    "- We have run a computationally expensive operation in parallel using a computing cluster\n",
    "- We have returned only the meaningful result (something that fits in memory) to our local machine\n",
    "- We have obtained a Python (pandas) object we can easily work with for the remainder of our analysis (plotting, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569dada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the type of this python variable\n",
    "type(num_muons_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd20d69",
   "metadata": {},
   "source": [
    "Produce the simple plot we were attempting to do in the previous cell:\n",
    "- an histogram of the number of muons per record, comparing data and simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac386f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use matplotlib to plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# fill the histogram with the simulation (mc)\n",
    "mc_counts = num_muons_dist.loc[num_muons_dist['sample']=='mc']\n",
    "plt.bar(\n",
    "    mc_counts['nMuon'], mc_counts['count'],\n",
    "    label = 'Montecarlo', width=1, alpha=0.6\n",
    ")\n",
    "\n",
    "# use an errorbar plot to overlay data\n",
    "# the error is the square root of the counts\n",
    "data_counts = num_muons_dist.loc[num_muons_dist['sample']=='data']\n",
    "plt.errorbar(\n",
    "    data_counts['nMuon'],data_counts['count'],\n",
    "    yerr = np.sqrt(data_counts['count']),\n",
    "    label = 'Data', color='black', fmt='o'\n",
    ")\n",
    "plt.xlim(-0.5,16.5)\n",
    "plt.xlabel(\"Number of muons\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46a0c3",
   "metadata": {},
   "source": [
    "#### Step 2 - Filter and flatten the dataset\n",
    "\n",
    "The end-goal of this task is to study dimuon events, i.e. events/records having exactly 2 muons. \n",
    "\n",
    "Selecting such records is a simple filtering task, that can be performed in a variety of ways using the Spark DataFrame API:\n",
    "- `pandas`-like selections, e.g. `df[df.column > xyz]`\n",
    "- `where` or `filter` Spark DataFrame operations, analogous to the `filter` se used with RDDs\n",
    "- fully-fledged `SQL` statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054481bb",
   "metadata": {},
   "source": [
    "`pandas`-like selections do work similarly to their pandas counterpart by operating on columns of the DataFrame:\n",
    "\n",
    "```python\n",
    "df[df.column > xyz]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only events with two muons\n",
    "dimuon_df = df[df['nMuon']==2]\n",
    "\n",
    "# print the first 5 rows\n",
    "dimuon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f55852",
   "metadata": {},
   "source": [
    "Also `where`/`filter` operations work similarly to the equivalent `pandas` logic.\n",
    "The two are alieases of the same function (specifically `where` is an alias of `filter`) therefore there is no difference in writing one or the other.\n",
    "\n",
    "We have already used `filter` on plain RDDs writing a Python function to apply to arbitrary data.\n",
    "\n",
    "Using `where`/`filter` with DataFrames and have it perform an operation on all items of a given colum is in principle a slightly different concept.\n",
    "\n",
    "Spark has to \"understand\" that we are working with a columnar object, and not with a plain collection of arbitrary data.\n",
    "This is what we refer to when we say that we have low-level APIs and high-level APIs. From the user point of view the method call is going to be the same, but the underlying \"wiring\" of the function is different (we are offered a similar _interface_ to different _inner functions_ in the two cases).\n",
    "\n",
    "We can write the same where/filter function in multiple ways:\n",
    "- specifying that we are operating on a column named `column`\n",
    "```python\n",
    "df.where(col('column')>xyz)\n",
    "```\n",
    "- recalling the column from the dataframe \n",
    "```python\n",
    "df.where(df['column']>xyz)\n",
    "```\n",
    "- using the column name in the filter field\n",
    "```python\n",
    "df.where(\"column > xyz\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad103e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the column object from pyspark\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# apply where explicitely on the spark column object\n",
    "df.where(col('nMuon')==2)\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d815fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply where recalling the column from the dataframe\n",
    "df.where(df['nMuon']==2)\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply where (or filter) using the column name in the filter field\n",
    "df.filter(\"nMuon == 2\")\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13860423",
   "metadata": {},
   "source": [
    "Spark DataFrame has an underlying optimization engine for structured filters based on SQL.\n",
    "\n",
    "Dealing with structured data, a common usecase is to use Spark to interface with SQL:\n",
    "- use SQL statements to query large and possibly distributed structured datasets\n",
    "- connect to a remote SQL database and process data in parallel\n",
    "- preprocess large amounts of raw data and then translate them into tables of a DB for data management\n",
    "- ...\n",
    "\n",
    "In PySpark using the structured data API (actually named `pyspark.sql` as we have seen) we can create a table _view_ from our dataset, and then use plain SQL code to run the same filters or aggregations.\n",
    "\n",
    "The underlying scheduling and task optimization of Spark is always going to be there, and will help execute queries and data filters fast and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66049d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a temporary table view of the dataframe \n",
    "# this will allow spark to execute SQL queries on the dataframe\n",
    "# treating the dataframe as a table in a RDBMS\n",
    "df.createOrReplaceTempView(\"dimuon_df_table\")\n",
    "\n",
    "# select all rows from the table\n",
    "# where the number of muons is 2\n",
    "spark.sql(\"\"\"\n",
    "          SELECT * FROM dimuon_df_table \n",
    "          WHERE nMuon = 2\n",
    "          \"\"\")\\\n",
    "     .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c4009",
   "metadata": {},
   "source": [
    "After filtering the dataset retaining only the data we want, we can start accessing the features and producing higher-level quantities and results.\n",
    "\n",
    "However, at this stage the Dataset is not flat, as we have nested content in the Muons column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the dataframe schema\n",
    "dimuon_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a9cf9",
   "metadata": {},
   "source": [
    "We can flatten our dataset for further processing: in this way we will be able to perform operations between columns as in a \"normal\" `pandas` DataFrame.\n",
    "\n",
    "A new column can be created by using `withColumn(name, func)` where :\n",
    "- `name` is the name to be assigned to the new column\n",
    "- `func` is a function taking as input one or more columns\n",
    "\n",
    "For instance, a function may be to multiply together values in different columns, or simply copying the elements of a nested structur into a plain column.\n",
    "\n",
    "However, any arbitrary complex function can be used to produce new columns: later we will define arbitrary **User Defined Functions (UDFs)** that will be applied to every row of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc92624",
   "metadata": {},
   "source": [
    "To refine our selection retaining only a subset of columns, we can use the `select` statement, similarly to the `SELECT` SQL keyword.\n",
    "\n",
    "On top of this, we can also drop all unused columns from the dataset with the `drop` function of the DataFrame.\n",
    "\n",
    "To recap:\n",
    "- `select()` projects a set of expressions and returns a new DataFrame\n",
    "- `withColumn()` creates a new attribute from the given column\n",
    "- `drop()` removes columns from the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c202548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from the dimuon dataframe (the filtered one)\n",
    "dimuon_flat = (\n",
    "    # select only the columns we need for the analysis\n",
    "    dimuon_df.select([col('sample'), col('nMuon'), col('Muons')[0], col('Muons')[1]])\n",
    "    # and then flatten the Muons array into one column per feature:\n",
    "    #  - Energy                E\n",
    "    #  - Momentum along x-axis px\n",
    "    #  - Momentum along y-axis py\n",
    "    #  - Momentum along a-axis pz\n",
    "    #  - Charge                c\n",
    "    .withColumn('E1', col('Muons[0].E'))\n",
    "    .withColumn('px1', col('Muons[0].px'))\n",
    "    .withColumn('py1', col('Muons[0].py'))\n",
    "    .withColumn('pz1', col('Muons[0].pz'))\n",
    "    .withColumn('c1',  col('Muons[0].charge'))\n",
    "    .withColumn('E2',  col('Muons[1].E'))\n",
    "    .withColumn('px2', col('Muons[1].px'))\n",
    "    .withColumn('py2', col('Muons[1].py'))\n",
    "    .withColumn('pz2', col('Muons[1].pz'))\n",
    "    .withColumn('c2',  col('Muons[1].charge'))\n",
    "    # drop the two Muons nested structures\n",
    "    .drop('Muons[0]', 'Muons[1]')\n",
    ")\n",
    "\n",
    "# print the schema of the new dataframe\n",
    "dimuon_flat.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7458de",
   "metadata": {},
   "source": [
    "#### Step 3 - Computing new features\n",
    "\n",
    "At this stage we are left with pairs of muons with flat data structures.\n",
    "\n",
    "We would like to select only those pairs of muons with opposite electric charge, and compute the invariant mass from the four-momenta of the two selected particles.\n",
    "\n",
    "This is a relativistic invariant property that resonates around a given value in the case thw two muons are produced by the decay of a \"parent particle\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from the flat dimuon dataset\n",
    "# select only the pairs with opposite charge\n",
    "dimuon_flat = \n",
    "\n",
    "# count the number of opposite-charge pairs\n",
    "dimuon_flat.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76711812",
   "metadata": {},
   "source": [
    "To compute the invariant mass of the two candidate muons we need first to compute the four-momentum of the parent particle \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "p &= \\left(E_1+E_2,\\quad p_{x,1}+p_{x,2},\\quad p_{y,1}+p_{y,2},\\quad p_{z,1}+p_{z,2}\\right) = \\\\\n",
    "  &= \\left(E,\\;p_x,\\;p_y,\\;p_z\\right)=\\\\\n",
    "  &= \\left(E,\\;\\mathbf{p}\\right)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Create four new columns in the dataframe, one per each of the four coordinates: $(E,~px,~py,~pz)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f541cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create four new columns for the four components of the momentum\n",
    "# and add them to the dataframe\n",
    "dimuon_flat = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367177bd",
   "metadata": {},
   "source": [
    "From the 4-momentum of the candidate we can compute the invariant mass as\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "M &= \\sqrt{p\\cdot p} =  \\sqrt{(E^2 - \\|\\mathbf{p}\\|^2)} =&\\\\\n",
    "  &= \\sqrt{E^2 - (px^2 + py^2 + pz^2)}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "We can compute it by manipulating directly the columns of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the invariant mass\n",
    "\n",
    "# import the spark sqrt function \n",
    "from pyspark.sql.functions import sqrt\n",
    "\n",
    "# create a new column for the invariant mass\n",
    "dimuon_flat = \n",
    "\n",
    "# persist the dataframe after these new columns have been added\n",
    "dimuon_flat\n",
    "\n",
    "# show the first 5 values of the M column of the dataframe\n",
    "dimuon_flat.select('M').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f3cab",
   "metadata": {},
   "source": [
    "As we have anticipated, we can define custom functions and use them to evaluate complex features from the columns of our DataFrame.\n",
    "\n",
    "A User Defined Function (UDF) is any arbitrary Python function that can be evaluated by Spark on any row of your DataFrame.\n",
    "If there is any risk a UDF might fail (e.g. if the value of a column is `None`) you must remember to protect the function (e.g. by adding a `if _ is not None` condition).\n",
    "\n",
    "- PySpark UDFs are extremely useful when **there is no native Spark function available for the computation you need to perform**.\n",
    "- They are also a nice way of writing **simple Python code** and let Spark run it on the data stored in the partitions.\n",
    "- We can easily **write and reuse complex functions** in multiple occurrencies in our distributed computation.\n",
    "\n",
    "***BUT...*** the PySpark UDFs are custom Python code executed on Java VMs (the executors)...\n",
    "Spark will thus have to start Python processes on the excutors, execute the function row by row on that data in the Python process, and then finally return the results of the row operations to the JVM and Spark.\n",
    "**This typically takes a toll on the performances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92205706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pyspark udf\n",
    "from pyspark.sql.functions import udf\n",
    "import math\n",
    "\n",
    "# create a function that takes in four columns and returns a float\n",
    "# this is the function that will be used as the udf\n",
    "@udf('float')\n",
    "def invariant_mass(E, px, py, pz):\n",
    "    # the function evaluates plain python code \n",
    "    # no vectorization (numpy) is involved\n",
    "    return math.sqrt(E*E - (px*px + py*py + pz*pz))\n",
    "\n",
    "# apply the udf to the appropriate columns\n",
    "# and return the first 5 rows\n",
    "dimuon_flat.select(\n",
    "    invariant_mass(col('E'), col('px'), col('py'), col('pz'))\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34c75a",
   "metadata": {},
   "source": [
    "To partially overcome the performance issues, we can use **Pandas UDFs in PySpark**, also referred to as **vectorized UDFs**.\n",
    "\n",
    "This allows to process PySpark's DataFrames and columns as Pandas DataDrames and Pandas series, respectively.\n",
    "\n",
    "Differently from the standard UDFs, these Pandas UDFs perform the same operation on the entire pandas series (vectorized operation) and do not run one-row-at-a-time.\n",
    "\n",
    "This provides a boost in the efficiency of the computation and thus should be preferred whenever there is the possibility.\n",
    "\n",
    "They result particularly efficient in the \"grouped maps\", i.e. by appling a user defined function after a groupby to perform dedicated aggregation on the data. \n",
    "\n",
    "More on this can be found [here](https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html#pandas-udfs-a-k-a-vectorized-udfs) and [here](https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html#grouped-map). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pyspark pandas udf\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# the pandas udf takes four pandas series\n",
    "# and returns a pandas series (a single column)\n",
    "@pandas_udf('float')\n",
    "def invariant_mass_udf(\n",
    "    # the four input pandas series (with explitic type definition)\n",
    "    E: pd.Series,\n",
    "    px: pd.Series,\n",
    "    py: pd.Series,\n",
    "    pz: pd.Series\n",
    ") -> pd.Series:\n",
    "    # the resulting pandas series (a numpy array)\n",
    "    # once again with the explicit type definition\n",
    "    return np.sqrt(E*E - (px*px + py*py + pz*pz))\n",
    "\n",
    "# apply the pandas udf to the appropriate columns\n",
    "# and return the first 5 rows\n",
    "#\n",
    "# we can also use an alias to rename the column\n",
    "dimuon_flat.select(\n",
    "    invariant_mass_udf(col('E'), col('px'), col('py'), col('pz')).alias('M pandas udf')\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c2293",
   "metadata": {},
   "source": [
    "Compute the average energy $E$ of the muons found in each of the two `sample` groups.\n",
    "\n",
    "To do this, run compare the two approaches:\n",
    "1. use only PySpark native functions to run groupBy and aggregations (look up [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/grouping.html) about all the available built-in aggregations)\n",
    "2. use a vectorized-UDF to evaluate the average for each group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75b03f",
   "metadata": {},
   "source": [
    "Use only PySpark native functions to run groupBy and aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f38c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean energy for muons in each \n",
    "# sample using pyspark native functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad534b7d",
   "metadata": {},
   "source": [
    "Use a vectorized-UDF to run the aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean energy for muons in each \n",
    "# sample using a pyspark vectorized udf function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d783a67",
   "metadata": {},
   "source": [
    "#### Step 4 - (Re-)Discovering particles\n",
    "\n",
    "Now that we have loaded our dataset in the cluster, inspected it, pre-processed it by selecting and flattening variables, and we have computed all the high-level quantities, we can finally use the dataset to perform the final analysis.\n",
    "\n",
    "For example, let's plot the invariant mass $M$ of the dimuon system in its whole range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the histogram of M\n",
    "# \n",
    "# 1. select the M column\n",
    "# 2. get the underlying RDD\n",
    "# 3. retrieve the M values into a histogram \n",
    "#    (remember the .histogram() RDD function from the previous notebook)\n",
    "# \n",
    "# - use 0-150 GeV as the histogram range\n",
    "# - use 2 GeV as the bin width\n",
    "\n",
    "bins, counts = dimuon_flat.\n",
    "\n",
    "# print the bins and counts to check the logic\n",
    "print(bins, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two histograms, one per sample (data/mc)\n",
    "\n",
    "# it could be convenient to use a dictionary \n",
    "# to host the two samples' bins and counts\n",
    "histogram_results = {}\n",
    "\n",
    "# perform the same pyspark function on both samples\n",
    "for sample in ['mc', 'data']:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71712c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot containing\n",
    "# - a filled histogram for the Simulation (mc)\n",
    "# - an errorbar plot for the Data (data), with sqrt(count) as the y-error\n",
    "# - show the resulting plot in semi-log scale\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "mc_res = histogram_results['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(bin_centers, \n",
    "         weights=mc_res['counts'], \n",
    "         bins=mc_res['bins'],\n",
    "         label='Simulation', \n",
    "         alpha=0.6\n",
    ")\n",
    "\n",
    "data_res = histogram_results['data']\n",
    "bin_centers = data_res['bins'][:-1] + np.diff(data_res['bins'])/2\n",
    "plt.errorbar(\n",
    "    bin_centers, \n",
    "    data_res['counts'], \n",
    "    yerr=np.sqrt(data_res['counts']),\n",
    "    fmt='o', \n",
    "    ms=4, \n",
    "    lw=1, \n",
    "    color='black', \n",
    "    label='Data'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$m_{\\mu \\mu}$ [GeV]\")\n",
    "plt.ylabel(f\"Events/{bin_width}GeV\")\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1892c",
   "metadata": {},
   "source": [
    "The plot shows a number of features we are not going to talk about in this class.\n",
    "\n",
    "However, to complete the exercise, we'll focus on the range 75-115 GeV, where it appears being a large resonance.\n",
    "\n",
    "We can create a new plot of data and simulations in that specific range with higher binning \"resolution\" (0.5 GeV), to study how well the simulation agrees with the experimenta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot containing\n",
    "# - a filled histogram for the Simulation (mc)\n",
    "# - an errorbar plot for the Data (data), with sqrt(count) as the y-error\n",
    "# - show the resulting plot in semi-log scale\n",
    "histogram_results = {}\n",
    "for sample in ['mc', 'data']:\n",
    "    histogram_results[sample] = {}\n",
    "    \n",
    "    bins, counts = dimuon_flat.where(col('sample')==sample)\\\n",
    "                              .select('M')\\\n",
    "                              .rdd\\\n",
    "                              .histogram(list(np.arange(70,115,0.5)))\n",
    "    \n",
    "    histogram_results[sample]['bins'] = bins\n",
    "    histogram_results[sample]['counts'] = counts\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "mc_res = histogram_results['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(bin_centers, \n",
    "         weights=mc_res['counts'], \n",
    "         bins=mc_res['bins'],\n",
    "         label='Simulation', \n",
    "         alpha=0.6\n",
    ")\n",
    "\n",
    "data_res = histogram_results['data']\n",
    "bin_centers = data_res['bins'][:-1] + np.diff(data_res['bins'])/2\n",
    "plt.errorbar(\n",
    "    bin_centers, \n",
    "    data_res['counts'], \n",
    "    yerr=np.sqrt(data_res['counts']),\n",
    "    fmt='o', \n",
    "    ms=4, \n",
    "    lw=1, \n",
    "    color='black', \n",
    "    label='Data'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$m_{\\mu \\mu}$ [GeV]\")\n",
    "plt.ylabel(f\"Events/0.5GeV\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a7328",
   "metadata": {},
   "source": [
    "We can try to perform some selection, to improve the quality of the signal and remove background, and hopefully this selection might help restoring a decent Data-Simulation agreement.\n",
    "\n",
    "To do this we can try look at dimuon candidates with a large transverse momentum \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "p_T &= \\sqrt{p_x^2 + p_y^2} = \\\\\n",
    "    &= \\sqrt{(p_{x,1}+p_{x,2})^2 + (p_{y,1}+p_{y,2})^2}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "- Create a colum with name `highpt` and for each row set the value to `True` if $p_T\\geq 30$ Gev and `False` otherwise\n",
    "    - Start from the two muons and use a UDF for this\n",
    "    - Use the PySpark `when` and `otherwise` function (have a look at the documentation [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.otherwise.html))\n",
    "- Plot the Data-Simulation comparison for dimuon candidates passing the `highpt` selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba342cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the udf to evaluate the dimuon pT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc981c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the udf to the dimuon_flat dataframe\n",
    "dimuon_flat = dimuon_flat.withColumn(\"pT\",\n",
    "                                     candidate_pt(col('px1'),col('py1'),col('px2'),col('py2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591ecb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the when function from pyspark.sql\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# create the `highpt` feature with where/otherwise\n",
    "dimuon_flat = dimuon_flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb08d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect a few rows of the dataframe \n",
    "# to verify the feature is set correctly\n",
    "dimuon_flat['sample','M','pT','highpt'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d446d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the two data-simulation comparison plots\n",
    "# for low- and high-pt dimuon candidate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53cb88c",
   "metadata": {},
   "source": [
    "## Stop worker and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ffaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e424f6",
   "metadata": {},
   "source": [
    "Finally, use `docker compose down` to stop and clear all running containers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
