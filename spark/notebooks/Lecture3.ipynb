{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb40c3f9",
   "metadata": {},
   "source": [
    "# Lecture 3: Spark Streaming\n",
    "\n",
    "_Spark Streaming_ is an extension of the Spark API that enables scalabe stream processing.\n",
    "\n",
    "The continous stream of input data can be ingested from many data sources such as **Kafka**, **Amazon s3** or **TCP sockets**. \n",
    "\n",
    "The Spark API allows to process data via high-level functions such as *map* and *reduce*. As we are going to see, it is also possible to use dataframe operations. \n",
    "\n",
    "Processed data can be exported to an external database and used to make live dashboards or offline analyses, or stored in files, or be used in a further stage of a Kafka pipeline. \n",
    "\n",
    "Spark streaming works by dividing the input data into _micro-batches_ that can be treated as static datasets. In Spark this is referred to as a *discretized stream* (*DStream*). The DStream is represented using RDDs.\n",
    "\n",
    "![DStream](imgs/lecture3/DStream.png)\n",
    "\n",
    "Any transformation applied on the DStream, i.e. anything like a `Dstream.map()`, will act independently on each batch. For example, in the image below, we can filter the original RDD to remove some data and produce a new stream. \n",
    "\n",
    "![DStream_filter](imgs/lecture3/Dstream_filter.png)\n",
    "\n",
    "In this lecture we will see how to setup a simple stream using a TCP socket as a data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77789660",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbb40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "# -> 'local'\n",
    "# -> 'docker_container'\n",
    "# -> 'docker_cluster'\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4330629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable\n",
    "%env CLUSTER_TYPE $CLUSTER_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccf872",
   "metadata": {},
   "source": [
    "## Start the cluster \n",
    "\n",
    "Environment variables need to be set only in the case of a local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc44b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLUSTER_TYPE=='local':\n",
    "    import findspark\n",
    "    findspark.init('/home/pazzini/work/courses/MAPD_B/MAPD-B/spark/spark-3.2.1-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231432e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    echo \"Launching master and worker\"\n",
    "    \n",
    "    # start master \n",
    "    $SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "        --port 7077 --webui-port 8080\n",
    "    \n",
    "    # start worker\n",
    "    $SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "        --cores 4 --memory 2g\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14513840",
   "metadata": {},
   "source": [
    "## Create the Spark session\n",
    "\n",
    "Later on we will explain what is the role of [Apache Arrow](), but first we need to install it and create the spark session with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d10fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://localhost:7077\")\\\n",
    "        .appName(\"Spark streaming application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # use the provided master\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .appName(\"Spark streaming application\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d6601",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b253c26",
   "metadata": {},
   "source": [
    "## Spark _Streaming_ context\n",
    "\n",
    "The first step of a Spark streaming application is the creation of a `StreamingContext`. \n",
    "\n",
    "It is a similar concept to the `sparkContext` but it requires to be initialized with some additional information on how to handle the micro-batches.\n",
    "\n",
    "The context can be initialized using `StreamingContext(SparkContext, batch_interval`). \n",
    "The `batch_interval` parameter represents the wall-time between two batches, i.e. the batch duration in seconds.\n",
    "\n",
    "There could be at most one `StreamingContext` for each Spark application. \n",
    "\n",
    "The processing start when `StreamingContext.start()` is called, and it can be stopped with `StreamingContext.stop()`. \n",
    "\n",
    "By default, if the `StreamingContext` is stoped without passing the `stopSparkContext=False` option, the sparkContext is also stopped (thus the application is closed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256cf2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# create a streaming context with a batch interval of 2 seconds\n",
    "ssc = StreamingContext(sc, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922cfde8",
   "metadata": {},
   "source": [
    "For this example spark will read data from a TCP socket. \n",
    "\n",
    "A dummy data stream is generated by a simple python program that can be found in `utils/producer.py`. \n",
    "When executed, the producer will write data on port `5555` of `localhost`. \n",
    "\n",
    "The dataset consists of fake credit card transactions.\n",
    "\n",
    "Have a look at the code from the producer before executing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat utils/producer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a375f",
   "metadata": {},
   "source": [
    "The producer will generate new records in the form of a random combination of:\n",
    "- firstname\n",
    "- lastname\n",
    "- amount\n",
    "- delta time\n",
    "- flag\n",
    "\n",
    "These information will be formatted into a .json data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d127aa0",
   "metadata": {},
   "source": [
    "To declare to Spark that the StreamingContext data source will be a TCP socket, located at a given `hostname` and `port`, we can use the following \n",
    "\n",
    "`socketTextStream(hostname, port)`\n",
    "\n",
    "Have a look at the StreamingContext documentation to see all the available options, at the [link](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a socket stream from the hostname where we collect data from\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    hostname = \"localhost\"\n",
    "    \n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    hostname = \"spark-master\"\n",
    "\n",
    "# and set the port to 5555\n",
    "socket_stream = ssc.socketTextStream(hostname, 5555)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad88b4",
   "metadata": {},
   "source": [
    "#### start the python producer.py script\n",
    "\n",
    "Depending on the Spark deployment mode for this exercise, do either of the following:\n",
    "\n",
    "**Local** \n",
    "- from a terminal, run the python script with the option `--hostname localhost`\n",
    "\n",
    "\n",
    "**Single Docker container**\n",
    "- from a terminal, identify the id of the container using `docker ps`\n",
    "- connect to the Docker container using the `docker exec -it <CONTAINER_ID> bash` command\n",
    "- from the docker container, run the python script with the option `--hostname localhost`\n",
    "  \n",
    "**Docker cluster**\n",
    "- from a terminal, identify the id of the `spark-master` container using `docker ps`\n",
    "- connect to the Docker container using the `docker exec -it <CONTAINER_ID> bash` command\n",
    "- from the docker container, run the python script with the option `--hostname spark-master`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163466ca",
   "metadata": {},
   "source": [
    "The first thing we need to to is load the json describing each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# use the map() transformation to apply the same function to all rdds\n",
    "# the function we want to run is the json.load() of the messages\n",
    "json_stream = socket_stream.map( \"\"\" --- \"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64806443",
   "metadata": {},
   "source": [
    "It is possible to print some elements of each batch with `pprint()`. This can be used to explore the RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c29264",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0ad3b",
   "metadata": {},
   "source": [
    "Start the computations with `ssc.start()` and stop with `ssc.stop(stopSparkContext=False)`. Remember that after this last instruction the streaming context must be defined again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21738d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b52cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7dc2a8",
   "metadata": {},
   "source": [
    "Now that we know we can stream data to Spark we would like to start performing basic operations in a distributed fashion.\n",
    "\n",
    "Once the SparkStreamingContext is stopped we have to re-create a new one, as the connection between the socket and Spark is lost.\n",
    "\n",
    "Before running the following cells we should therefore:\n",
    "1. create a new `scc` object\n",
    "2. point it to the proper TCP socket and port\n",
    "3. start again the python producer application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Spark StreamingContext with a batch wall-time of 2 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c9ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the socket stream using the appropriate endpoint and port\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894abd4",
   "metadata": {},
   "source": [
    "#### start once again the python producer script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86587a4a",
   "metadata": {},
   "source": [
    "We now start listening on the TCP socket, interpreting the input data stream as json loads.\n",
    "\n",
    "Remember to get rid of the `pprint()` action, that would otherwise be performed continously, dumping the input data into the Jupyter cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a86004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new json_stream object by reading the json loads from the socket\n",
    "json_stream = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026ab834",
   "metadata": {},
   "source": [
    "We may want to convert each batch into a Spark DataFrame to have access to the higher level APIs. \n",
    "\n",
    "In order to do that, let's first convert all the numeric features of the json into python floats and integers variables. \n",
    "This is a simple type cast operation in python, that can be easily parallelized.\n",
    "\n",
    "The python dictionary produced by each json message, re-casted with the proper data types, can then be used to create a `Row`for each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0782d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# create a row for each message \n",
    "#   convert each numerical value to the proper python type\n",
    "#   create a row from each message\n",
    "def create_row_rdd(t):\n",
    "    ...\n",
    "    \n",
    "    return Row(**t)\n",
    "\n",
    "# apply the transformation to the json_stream rdd\n",
    "row_stream = json_stream.map(create_row_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceaa05f",
   "metadata": {},
   "source": [
    "The method `DStream.foreachRDD` can be used to apply custom transformations to each *batch* of data. \n",
    "\n",
    "In our case, we are insterested in converting each batch of data into a Spark dataframe and perform operations, such as finding the number of transactions for each user. \n",
    "\n",
    "For the scope of this simple use-case we can consider that all batches where a user has performed more than one transaction with the `flag` field equal to one can be identified as fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if left unconstrained, Spark might want to create a very \n",
    "# large number of partitions for this streaming application\n",
    "# \n",
    "# using way more partitions than necessary always results in\n",
    "# a huge over-head due to the partition-to-partition communications\n",
    "# \n",
    "# this line is a trick to force Spark to use a small number of partitions\n",
    "# thus making it more efficient in the case of small workloads and few executors\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit, countDistinct\n",
    "\n",
    "# process each bach to identify possibly fraudolent transactions\n",
    "#   1. convert the rdd into a dataframe (provide the schema if necessary)\n",
    "#   2. compute the number of transactions per batch per user (a unique combination of first_and_lastname)\n",
    "#   3. identify all the \"suspicios\" transactions by counting the number of transactions from a unique user \n",
    "#   3. print the resulting dataframe to mimick a live reporting of the suspicious transactions\n",
    "\n",
    "def process_batch(rdd):\n",
    "    # convert rdd to df\n",
    "    #   check the documentation and/or the Lecture2 notebook \n",
    "    #   for details on how to create and pass a schema to a dataframe   \n",
    "    df = ...\n",
    "    \n",
    "    # find number of transactions for each user when flag = 1 \n",
    "    #    declare a new column to create a unique user identifier \n",
    "    #    this can be easily done by concatenating first- and last-name fields\n",
    "    #    check the concat function from pyspark.sql.functions \n",
    "    num_transactions = ...\n",
    "    \n",
    "    # find suspicious transactions\n",
    "    #    filter only users with more than one transaction per batch\n",
    "    #    create a \"fraud\" column with a value of 1 for the selected users (check the lit function)\n",
    "    #    from the dataframe, project only the unique id and fraud columns\n",
    "    sus_transactions = ...\n",
    "    \n",
    "    # (trigger an automatic alert)\n",
    "    #Â print the first 5 items of the resulting dataframe\n",
    "    sus_transactions. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a7758",
   "metadata": {},
   "source": [
    "Finally, instruct Spark to execute this `process_batch` function for each RDD you will have in your DStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_stream.foreachRDD(process_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad010131",
   "metadata": {},
   "source": [
    "Now you should be ready to start the spark streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9ee85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start streaming context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834a208",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# stop streaming context - no not stop the SparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d3e64",
   "metadata": {},
   "source": [
    "## Stop worker and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    # stop worker \n",
    "    $SPARK_HOME/sbin/stop-worker.sh\n",
    "    \n",
    "    # start master\n",
    "    $SPARK_HOME/sbin/stop-master.sh\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e1abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
