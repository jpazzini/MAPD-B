{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb40c3f9",
   "metadata": {},
   "source": [
    "# Lecture 3: Spark Streaming\n",
    "\n",
    "_Spark Streaming_ is an extension of the Spark API that enables scalabe stream processing.\n",
    "\n",
    "The continous stream of input data can be ingested from many data sources such as **Kafka**, **Amazon s3** or **TCP sockets**. \n",
    "\n",
    "The Spark API allows to process data via high-level functions such as *map* and *reduce*. As we are going to see, it is also possible to use dataframe operations. \n",
    "\n",
    "Processed data can be exported to an external database and used to make live dashboards or offline analyses, or stored in files, or be used in a further stage of a Kafka pipeline. \n",
    "\n",
    "Overall, the practice of reading data from a set of sources, pre-process it, and then store it in a different format for later analysis is extremely common, and has its own name: **realtime ETL pipelines**.\n",
    "- **E**xtract\n",
    "- **T**transform\n",
    "- **L**oad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9d87b",
   "metadata": {},
   "source": [
    "Spark streaming works by dividing the input data into _micro-batches_ that can be treated as static datasets. In Spark this is referred to as a *discretized stream* (*DStream*). The DStream is represented using RDDs.\n",
    "\n",
    "![DStream](imgs/lecture3/DStream.png)\n",
    "\n",
    "Any transformation applied on the DStream, i.e. anything like a `Dstream.map()`, will act independently on each batch. For example, in the image below, we can filter the original RDD to remove some data and produce a new stream. \n",
    "\n",
    "![DStream_filter](imgs/lecture3/Dstream_filter.png)\n",
    "\n",
    "In this lecture we will see how to setup a simple stream using a TCP socket as a data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14513840",
   "metadata": {},
   "source": [
    "## Create and Start a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d10fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build a SparkSession \n",
    "#   connect to the master node on the port where the master node is listening (7077)\n",
    "#   declare the app name \n",
    "#   configure the executor memory to 512 MB\n",
    "#   either *connect* or *create* a new Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"My streaming spark application\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d6601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# print its status\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b253c26",
   "metadata": {},
   "source": [
    "## Spark _Streaming_ context\n",
    "\n",
    "The first step of a Spark streaming application is the creation of a `StreamingContext`. \n",
    "\n",
    "The `StreamingContext` is a crucial component in Spark Streaming. It's responsible for initializing the Spark Streaming application and specifying how to handle micro-batches of data. \n",
    "\n",
    "The `StreamingContext` is a similar concept to the `sparkContext` but it requires to be initialized with some additional information to know how to handle the micro-batches.\n",
    "\n",
    "To create a `StreamingContext`, you can use the `StreamingContext(SparkContext, batch_interval)` constructor. The `SparkContext` object provides the necessary environment for Spark Streaming, while the `batch_interval` parameter determines the (wall-time) duration of each batch in seconds.\n",
    "\n",
    "It's important to note that you can only have at most **one** `StreamingContext` for each Spark application. Attempting to create multiple `StreamingContext` objects in a single application will result in errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62435f31",
   "metadata": {},
   "source": [
    "Create a Spark `StreamingContext` with a batch interval of 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256cf2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# create a streaming context with a batch interval of 2 seconds\n",
    "ssc = StreamingContext(sc, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530aaeae",
   "metadata": {},
   "source": [
    "### Starting and Stopping Spark Streaming\n",
    "\n",
    "To process data in real-time using Spark, we need to create a `StreamingContext`, define the operations to perform on the data, and specify the data source and sink to connect to.\n",
    "\n",
    "Once the streaming operations are defined, we can start processing the stream by calling the `.start()` method of the `StreamingContext` object (`ssc` in our case). Similarly, we can stop the streaming processing by calling the `.stop()` method.\n",
    "\n",
    "**NOTE:** It's important to note that when we stop the `StreamingContext`, the default behavior is to also stop the `SparkContext`. This means that the entire Spark application will be closed by default. To prevent this, we can pass the `stopSparkContext=False` option when stopping the `StreamingContext`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922cfde8",
   "metadata": {},
   "source": [
    "### TCP Socket Source\n",
    "\n",
    "For this example spark will read data from a TCP socket using Spark Streaming.\n",
    "\n",
    "A TCP socket is a communication endpoint used to establish a connection between two devices over a network.\n",
    "You can think of it as a telephone connection: two endpoints have to enstablish a connection; once the connection is enstablished, a communication can occur, with a data transfer; as soon as one of the two ends interrupts the connection the whole communcation is lost. \n",
    "\n",
    "We will generate a dummy data stream representing fake credit card transactions.\n",
    "\n",
    "A simple python program will be used to create this data stream.\n",
    "You will be able to find it in `utils/producer.py`. \n",
    "When executed, the producer will try to enstablish a TCP connection and send data on port `5555` of a given `host` (`spark-master` in our case). \n",
    "\n",
    "Before executing the producer program, take a moment to review the `producer.py` code to understand how it works. It's important to understand the logic of the program before using it to generate the streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat utils/producer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a375f",
   "metadata": {},
   "source": [
    "The producer will generate new records in the form of a random combination of:\n",
    "- `name`\n",
    "- `surname`\n",
    "- `amount`: amount of the credit card transaction\n",
    "- `delta_t`: time between transactions\n",
    "- `flag`: random flag to indicate if potentially fraudolent or not\n",
    "\n",
    "These information will be formatted into a `.json` data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d127aa0",
   "metadata": {},
   "source": [
    "### Declaring the `StreamingContext` data source as a TCP socket\n",
    "\n",
    "To inform Spark that the StreamingContext data source will be a TCP socket located at a specific `hostname` and `port`, we can use the `socketTextStream(hostname, port)` method.\n",
    "\n",
    "Refer to the [StreamingContext documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.html) for additional available options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hostname and port number\n",
    "hostname = \"spark-master\"\n",
    "portnumber = 5555\n",
    "\n",
    "# declare the Spark Streaming source as TCP socket \n",
    "socket_stream = ssc.socketTextStream(hostname, portnumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad88b4",
   "metadata": {},
   "source": [
    "### Start the python producer.py script\n",
    "\n",
    "From a terminal/WSL, connect to the `spark-master` Docker container using the command\n",
    "```bash\n",
    "docker exec -it spark-master bash\n",
    "``` \n",
    "\n",
    "From inside the docker container, move to the `/mapd-workspace` folder and execute the python script with the option `--hostname spark-master`:\n",
    "\n",
    "```bash\n",
    "python notebooks/utils/producer.py --hostname spark-master\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163466ca",
   "metadata": {},
   "source": [
    "## Exploring the data stream\n",
    "\n",
    "The first thing we need to to is load the data describing each transaction, formatted as `json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# use the map() transformation to apply the same function to all rdds\n",
    "# the function we want to run is the json.loads() of the messages\n",
    "json_stream = socket_stream.map(\"\"\" --- \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64806443",
   "metadata": {},
   "source": [
    "It is possible to print some elements of each batch with `pprint()`. This can be used to explore the RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c29264",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0ad3b",
   "metadata": {},
   "source": [
    "**Start the computations with `ssc.start()` and stop with `ssc.stop(stopSparkContext=False)`.** \n",
    "\n",
    "_Remember that once the StreamingContext has been stopped, it must be redefined anew if we want to restart the streaming computations._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21738d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b52cdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8962c5",
   "metadata": {},
   "source": [
    "## Working with Streaming data\n",
    "\n",
    "Now that we know how to stream data into Spark, let's explore how we can perform basic distributed operations on the data.\n",
    "\n",
    "However, before we can proceed, we need to make sure that we have properly restarted the `StreamingContext` object, as the connection between the socket and Spark will be lost when the context is stopped.\n",
    "\n",
    "To restart the streaming context, we need to:\n",
    "1. Create a new `StreamingContext` object (we can reuse the `ssc` object in our case).\n",
    "2. Point it to the correct TCP socket and port where the data is being streamed from.\n",
    "3. Restart the Python producer application.\n",
    "\n",
    "Once the `StreamingContext` is properly set up and running, we can start applying distributed operations to the streaming data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b929f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Spark StreamingContext with a batch wall-time of 2 seconds\n",
    "ssc = \"\"\" --- \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b1a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the socket stream using the appropriate endpoint and port\n",
    "socket_stream = \"\"\" --- \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7960311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the python producer script\n",
    "### from the terminal/WSL shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb5cc3",
   "metadata": {},
   "source": [
    "We now start listening on the TCP socket, interpreting the input data stream as `json` loads.\n",
    "\n",
    "**Remember to get rid of the `pprint()` action, that would otherwise be performed continously, dumping the input data into the Jupyter cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75fce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new json_stream object by reading the json loads from the socket\n",
    "json_stream = socket_stream.map(\"\"\" --- \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f25ac",
   "metadata": {},
   "source": [
    "#### Converting Streaming Data to a DataFrame\n",
    "\n",
    "To make use of Spark's higher-level APIs, we can convert each batch of streaming data into a DataFrame. \n",
    "\n",
    "To do so, we'll first need to convert the numeric features of the incoming JSON data into Python floats and integers. This is a simple type cast operation that can be easily parallelized.\n",
    "\n",
    "After casting the data, we can create a `Row` object for each transaction using the resulting Python dictionary. These `Row` objects can then be used to create a DataFrame, allowing us to use Spark's higher-level APIs for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0782d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# create a row for each message \n",
    "#   convert each numerical value to the proper python type\n",
    "#   create a row from each message\n",
    "def create_row_rdd(t):\n",
    "    t['amount'] = float(t['amount'])\n",
    "    t['delta_t'] = float(t['delta_t'])\n",
    "    t['flag'] = int(t['flag'])\n",
    "    \n",
    "    return Row(**t)\n",
    "\n",
    "# apply the transformation to the json_stream rdd\n",
    "row_stream = json_stream.map(create_row_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceaa05f",
   "metadata": {},
   "source": [
    "The method `DStream.foreachRDD` can be used to apply custom transformations to each *batch* of data. \n",
    "\n",
    "In our case, we are insterested in converting each batch of data into a Spark DataFrame and perform operations, such as counting the number of transactions for each user. \n",
    "\n",
    "In this specific use-case, we can identify batches where a user has performed more than one transaction with the `flag` field equal to one as fraudulent. For simplicity, we will assume that these batches represent fraudulent activity.\n",
    "\n",
    "In reality, this might be a flag you might set on the fly using statig-rules or a ML-based model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8774b",
   "metadata": {},
   "source": [
    "**NOTE**: If left unconstrained, Spark might want to create a very large number of partitions for this streaming application.\n",
    "\n",
    "Using way more partitions than necessary always results in a huge over-head due to the partition-to-partition communications.\n",
    "\n",
    "We can force Spark to use a small yet reasonable (given the problem and resources we have) number of partitions\n",
    "thus making it more efficient in the case of small workloads and few executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line is a trick to force Spark to use a small number of partitions (4 in this example)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404abdd5",
   "metadata": {},
   "source": [
    "### Process each bach to identify possibly fraudolent transactions\n",
    "\n",
    "\n",
    "1. convert the RDD into a DataFrame (provide the schema if necessary)\n",
    "2. compute the _number of flagged transactions per batch per user_ (create a unique `userID` field as the combination of _FirstLastname_ to idenfity individual users)\n",
    "3. identify all the \"suspicios\" transactions per user: all users with more than one flagged transaction per batch will be assigned a `isFraud` boolean variable\n",
    "4. format the resulting `userID` and `isFraud` information in a DataFrame to mimick a \"live-report\" of the suspicious transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit, countDistinct\n",
    "\n",
    "def process_batch(rdd):\n",
    "    # convert rdd to df\n",
    "    #   check the documentation and/or the Lecture2 notebook \n",
    "    #   for details on how to create and pass a schema to a dataframe   \n",
    "    df = \"\"\" --- \"\"\"\n",
    "    \n",
    "    # find number of transactions for each user when flag = 1 \n",
    "    #    declare a new column to create a unique user identifier \n",
    "    #    this can be easily done by concatenating first- and last-name fields\n",
    "    #    check the concat function from pyspark.sql.functions \n",
    "    num_transactions = \"\"\" --- \"\"\"\n",
    "    \n",
    "    # find suspicious transactions\n",
    "    #    filter only users with more than one transaction per batch\n",
    "    #    create a \"fraud\" column with a value of 1 for the selected users (check the lit function)\n",
    "    #    from the dataframe, project only the unique id and fraud columns\n",
    "    sus_transactions = \"\"\" --- \"\"\"\n",
    "    \n",
    "    # (trigger an automatic alert)\n",
    "    #Â print the first 5 items of the resulting dataframe\n",
    "    sus_transactions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc37b0",
   "metadata": {},
   "source": [
    "Finally, instruct Spark to execute this `process_batch` function **for each RDD** you will have in your DStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac381b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_stream.foreachRDD(process_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51b635",
   "metadata": {},
   "source": [
    "Now you should be ready to start the spark streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9ee85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834a208",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# stop streaming context\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ded7c6",
   "metadata": {},
   "source": [
    "## Stop worker and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328e8cd",
   "metadata": {},
   "source": [
    "Finally, use `docker compose down` to stop and clear all running containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eac27c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
