{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "# -> 'local'\n",
    "# -> 'docker_cluster'\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = ''\n",
    "\n",
    "if CLUSTER_TYPE == 'local':\n",
    "\n",
    "    KAFKA_HOME = '<PATH_TO_YOUR_kafka_2.13-2.7.0_FOLDER>'\n",
    "    KAFKA_BOOTSTRAP_SERVERS = ['localhost:9092']\n",
    "    \n",
    "    # Start Zookeeper    \n",
    "    os.system('{0}/bin/zookeeper-server-start.sh {0}/config/zookeeper.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "    # Start one Kafka Broker\n",
    "    os.system('{0}/bin/kafka-server-start.sh {0}/config/server.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "\n",
    "    KAFKA_BOOTSTRAP_SERVERS = ['kafka-broker:9092']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Kafka Producer from shell\n",
    "\n",
    "Apache Kafka provides a set of bash scripts to interact and operate with the cluster for basic operations and testing such as:\n",
    "- topics creation, configuration and inspection\n",
    "- shell-based message producer \n",
    "- shell-based message consumer\n",
    "- shell-based performance testing\n",
    "- ...\n",
    "\n",
    "Let's first operate with the kafka cluster from shell by connecting to the broker and issuing a shell commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For docker_cluster users\n",
    "```console\n",
    "$ docker ps\n",
    "\n",
    "!!! replace your_kafka-broker_procid with your own kafka-broker process id\n",
    "!!!                          ||| \n",
    "!!!                          vvv  \n",
    "$ docker exec -it <your_kafka-broker_procid> /bin/sh \n",
    "\n",
    "# ./bin/bash\n",
    "# cd /usr/bin/kafka_2.13-2.7.0/bin\n",
    "# ls\n",
    "# ./kafka-topics.sh --create --topic my_awesome_topic --bootstrap-server kafka-broker:9092                \n",
    "# ./kafka-topics.sh --list --bootstrap-server kafka-broker:9092\n",
    "# ./kafka-topics.sh --describe --topic my_awesome_topic --bootstrap-server kafka-broker:9092\n",
    "# ./kafka-console-producer.sh --topic my_awesome_topic --bootstrap-server kafka-broker:9092\n",
    "```\n",
    "\n",
    "#### For local/VBox users\n",
    "```console\n",
    "\n",
    "!!! replace KAFKA_HOME with your own path to kafka_2.13-2.7.0 folder\n",
    "!!!     ||| \n",
    "!!!     vvv  \n",
    "$ cd KAFKA_HOME/bin \n",
    "\n",
    "$ ls \n",
    "$ ./kafka-topics.sh --create --topic my_awesome_topic --bootstrap-server localhost:9092                \n",
    "$ ./kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "$ ./kafka-topics.sh --describe --topic my_awesome_topic --bootstrap-server localhost:9092\n",
    "$ ./kafka-console-producer.sh --topic my_awesome_topic --bootstrap-server localhost:9092\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you should be able to send messages to the topic you just created via the kafka-console-producer.\n",
    "\n",
    "So far, no consumer is available to process or even display those messages... \n",
    "Yet the messages are succesfully sent to the topic, increasing the log(s) in the (possibly more than one) partition(s).\n",
    "\n",
    "Let's create a console consumer and subscribe to the topic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For docker_cluster users\n",
    "```console\n",
    "# ./kafka-console-consumer.sh --topic my_awesome_topic --bootstrap-server kafka-broker:9092 [--from-beginning]\n",
    "```\n",
    "\n",
    "#### For local/VBox users\n",
    "```console\n",
    "$ ./kafka-console-consumer.sh --topic my_awesome_topic --bootstrap-server localhost:9092 [--from-beginning]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kafka-python Producer\n",
    "\n",
    "Various python modules are available to interact with kafka, including:\n",
    "- kafka-python\n",
    "- confluent-kafka-python\n",
    "- pyKafka\n",
    "\n",
    "We'll use kafka-python to handle topics and producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[K     |████████████████████████████████| 246 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "! pip install kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka producers can be instantiated via the KafkaProducer class\n",
    "\n",
    "```python\n",
    "#--- A TYPICAL PRODUCER\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['62.30.10.23:9092'],  #<<<--- list of brokers\n",
    "    security_protocol=\"SSL\",                 #<<<--- security protocol (if any) \n",
    "    ssl_cafile=\"./ca.pem\",                   #<<<--- certificate details (if any)\n",
    "    ssl_certfile=\"./service.cert\",           #           ...\n",
    "    ssl_keyfile=\"./service.key\",             #           ...\n",
    "    value_serializer=msgpack.dumps           #<<<--- message value serialization function (e.g. interpred the message as a specific format)\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "We'll play with the vanilla version of the producer.\n",
    "No certificates or specific serialization is used in this example.\n",
    "\n",
    "A simple producer instantiated by pointing it to the kafka brokers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to publish a message to the topic we previously created without specifying any given key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.send('my_awesome_topic', b'message 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output message `<kafka.producer.future...>` is telling us explicitely that the record has been created and will be sent.\n",
    "However it has not been sent just yet...\n",
    "\n",
    "`KafkaProducer.send()` is in fact an asynchronous publish method.\n",
    "\n",
    "This means that the producer will enqueue the message on an internal queue which is later (after a tunable max buffering time / given number of messages) sent to the broker if a leader is available, else wait some more time for it to respond.\n",
    "\n",
    "This behaviour is perfectly OK. Even more, it's the expected behavoiur of kafka given the default settings.\n",
    "\n",
    "Just be aware that the messages won't be sent right away.\n",
    "If a large message rate is sent and `exit()` is issued right after it, it might by that no message is actually sent (because the max of the buffering time/n.msg is not reached).\n",
    "\n",
    "Have a look at the API for all the tunable parameters: https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html\n",
    "\n",
    "\n",
    "To send a message \"synchronously\" it can be issued a `flush()` of the producer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.send('my_awesome_topic', b'a new message')\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to realize that producers and consumers are completely decoupled. \n",
    "Even if a producer dies the consumer won't be affected by it, as it will still be able to access the topic on the brokers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "producer.send('my_awesome_topic', b'a message from the revived producer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messages have a `<key, value>` pair data structure.\n",
    "\n",
    "So far we have produced only messages with a given `value` but a `key` can be added as well.\n",
    "(message keys can be used also to point messages to specific partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.send(topic='my_awesome_topic', key=b'some_key', value=b'a message with key')\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a topic from kafka-python\n",
    "\n",
    "Kafka-python allows to admin the kafka cluster by defining new topics, and assinging then specific configuration parameters, such as the replication factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "kafka_admin = KafkaAdminClient(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the list of topics present on the cluster.\n",
    "\n",
    "This is the equivalent of issuing `./kafka-topics.sh --list --bootstrap-server kafka-broker:9092`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics are partitioned entities.\n",
    "Within each partition events are added to the end of the log, resulting in an ordered list of records.\n",
    "\n",
    "Publishing a new message to a partitioned topic will result in the addition of the message to the end of the log retained on the owner of a specific partition. If replication is enabled, the message will be then ridistributed to the other follower partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new topic explicitely\n",
    "#    w/   2 partitions\n",
    "#    w/o  replication \n",
    "a_new_topic = NewTopic(name='a_partitioned_topic', \n",
    "                       num_partitions=2, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[a_new_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish messages for the Spark Structured Streaming example\n",
    "\n",
    "Kafka can be used as a source for incoming messages in Spark Streaming and Structured Streaming.\n",
    "\n",
    "In Spark 3.1.1 the kafka integration is unfortunately not available for pySpark Streaming (while is still available for scala and java).\n",
    "\n",
    "We'll use the pySpark Structured Streaming API for implementing the example previously seen in the Spark hands-on sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "first_names=('John','Andy','Joe','Alice')\n",
    "last_names=('Johnson','Smith','Jones', 'Millers')\n",
    "\n",
    "# while 1:\n",
    "for i in range(20):\n",
    "    msg = {\n",
    "        'name': random.choice(first_names),\n",
    "        'surname': random.choice(last_names),\n",
    "        'amount': '{:.2f}'.format(random.random()*1000),\n",
    "        'delta_t': '{:.2f}'.format(random.random()*10),\n",
    "        'flag': random.choices([0,1], weights=[0.8, 0.2])[0]\n",
    "    }\n",
    "    producer.send('a_partitioned_topic', json.dumps(msg).encode('utf-8'))\n",
    "    producer.flush()\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new topic where to store the results of the Kafka+Spark processing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_new_topic = NewTopic(name='results', \n",
    "                       num_partitions=2, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[a_new_topic])\n",
    "\n",
    "kafka_admin.list_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
