{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of parallel and distributed computing with DASK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap on task parallelization (this time using Dask)\n",
    "\n",
    "We should now start to feel quite familiar with the programming pattern already discussed in the previous lectures when we discussed Hadood MapReduce and the Spark framework.\n",
    "\n",
    "Asides from the details of the API, which depend on the framework implementation, and from the inner structure of the job scheduling and resorce manager, most of the distributed computing frameworks offer us a way of parallelizing our jobs by means of Graph schedulers and Task optimizers.\n",
    "\n",
    "The goal is always the same, no matter the tool used to reach it:\n",
    "- store large datasets which won't fit into memory in a set of partitions \n",
    "- first plan the entire data processing, then optimize it to break it into smaller tasks\n",
    "- finally, distribute the processing close to the data, instead of moving the data (data-locality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Dask, we still have access to a set of dataset representations:\n",
    "- Dask bag (~ equivalent to the Spark RDD)\n",
    "- Dask array\n",
    "- Dask dataframe (~ equivalent to the Spark DataFrame)\n",
    "\n",
    "However, Dask is mainly a scheduler (this time written in Python and not in Scala as Spark \\[*\\]), which allows lazy execution of python-like code for distributing it across a multitude of workers.\n",
    "\n",
    "\n",
    "\\[*\\] To be clear, the fact that the scheduler is in Python does not mean it's _better_ than Spark for our purposes.  Python and Scala/Java are different in a multitude of ways, and both have pros and cons. For instance, as of today, it is known that due to the way Python handles and manages memory during the execution of an application, Dask still suffers from a issues with so-called unmanaged memory throughout the life of an application (see for instance the [link](https://coiled.io/blog/tackling-unmanaged-memory-with-dask/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a cluster and first lazy computation with `delayed`\n",
    "\n",
    "We can use Dask locally, to exploit the multi-tasking/processing functionalities, or setup a cluster and deploy a scheduler and a number of worker nodes.\n",
    "\n",
    "After we setup a cluster, we initialize a Client by pointing it to the address of a Scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "\n",
    "# -> 'local'\n",
    "# -> 'docker_container'\n",
    "# -> 'docker_cluster'\n",
    "\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%env CLUSTER_TYPE $CLUSTER_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out script_out\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    echo \"Launching scheduler and worker\"\n",
    "    \n",
    "    HOSTIP=`hostname -I | xargs`\n",
    "    \n",
    "    echo \"dask-scheduler --host $HOSTIP --dashboard-address $HOSTIP:8787\"\n",
    "    \n",
    "    # dask scheduler \n",
    "    dask-scheduler --host $HOSTIP --dashboard-address $HOSTIP:8787 &\n",
    "\n",
    "    # dask worker\n",
    "    dask-worker $HOSTIP:8786 --memory-limit 2GB --nworkers 2 &\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ip = !hostname -I | xargs\n",
    "host_ip = host_ip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "if CLUSTER_TYPE == 'local':\n",
    "    \n",
    "    client = Client()\n",
    "\n",
    "elif CLUSTER_TYPE == 'docker_container':\n",
    "    \n",
    "    client = Client('{}:8786'.format(host_ip))\n",
    "    \n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # use the provided master\n",
    "    client = Client('dask-scheduler:8786')\n",
    "    \n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status of the cluster at the `localhost:8787` dashboard, where we have an overview of both the status of the workers, and the execution of the DAG task graphs, when we'll start using them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest example to parallelize any arbitrary Python code in Dask can be presented by running a couple of simple arbitrary operarions, described here with 2 functions and an arbitrary sleep time of 1 second.\n",
    "The sleep time is meant to represent an arbitrariliy complex code, and the time required for its execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def increment(x):\n",
    "    # sleep for 1s\n",
    "    sleep(1)\n",
    "    \n",
    "    # given the input x, return x+1\n",
    "    return x + 1\n",
    "\n",
    "def decrement(x):\n",
    "    # sleep for 1s\n",
    "    sleep(1)\n",
    "\n",
    "    # given the input x, return x-1\n",
    "    return x - 1\n",
    "\n",
    "\n",
    "def add(x, y):\n",
    "    # sleep for 1s\n",
    "    sleep(1)\n",
    "    \n",
    "    # given the inputs x and y, return x+y\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the functions locally by running them on the client (not on the cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x = increment(1)\n",
    "y = decrement(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to let Dask take advantage of the processing units assigned to the cluster, we have to let it elaborate the DAG corresponding to the execution of the code we want to deploy on the cluster, and then let the scheduler dispatch the tasks to the workers.\n",
    "\n",
    "This is done in Dask by declaring that a function is `delayed`.\n",
    "\n",
    "The `delayed` Dask method takes several arguments. \n",
    "The first argument is the function that has to be executed in parallel.\n",
    "The following arguments are the arguments of the original function.\n",
    "\n",
    "We now want to transform the `inc` and `add` functions, therefore making them `lazy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x = delayed(increment)(1)\n",
    "y = delayed(decrement)(2)\n",
    "z = delayed(add)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, as usual with lazy operations, we don't have the results stored in `z`.\n",
    "\n",
    "`z` at this stage is just the \"plan\" of the code execution, made by the DAG task scheduler.\n",
    "\n",
    "We can visualize the plan of the execution with the `visualize()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to actually execture the job, we have to ask Dask to return the results of the lazy operation with the `compute()` method.\n",
    "\n",
    "Under the carpet, Dask will ship the computational graph to the scheduler, and will dispatch the tasks to the workers, similarly to what already discussed for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "z.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futures and Eager computation with `submit`\n",
    "\n",
    "The eager operation alternative to `delayed` in Dask is the `submit`, which instruct Dask to start executing our task on the cluster right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = client.submit(increment, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `increment` with its agurment `1` is submitted to the cluster, and the `future` object is returned right away. \n",
    "\n",
    "The `submit` operation has in fact returned a so-called execution *promise* (the `future` variable) of the instruction that we have submitted.\n",
    "It doesn't necessarily mean that the execution has been completed, as the cluster might take some time to execute it.\n",
    "\n",
    "The `future` variable in fact **doen not contains the result**, but just the promise of it, when the elaboration will be completed. \n",
    "The result of the computation will be left on the worker nodes of our cluster, and will not be sent back to our client until we reclaim it.\n",
    "\n",
    "To retrieve the result we do invoke the `gather` function (similarly to the collect in Spark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be extremely useful in situations where we need to submit an instruction multiple times, perhaps with different input parameters. \n",
    "This is a for instance a quite frequent scenario in in machine learning, for example when we need to optmize an algorithm for a particular dataset over the hyper-parameter space. \n",
    "\n",
    "The idea of this approach allows us to `map` the instruction that we want to executo to each argument of a dataset, thus submitting the same operation on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "future_results = client.map(increment, data)\n",
    "future_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = client.gather(future_results)\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to wait for the result of a `submit` computation to be ready (say we need it as an input in other computations), we may still ask Python to block the execution of the new code and `wait` the computation of the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "\n",
    "new_future = client.map(increment, new_data)\n",
    "wait(new_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(new_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In complete analogy to what we have discussed with the `delayed` lazy execution, we can combine multiple instruction that have to be submitted to the cluster and create a more complex job to run on our cluster.\n",
    "\n",
    "We should remember that the results of the `submit` execution reside on the cluster until a `gather` is used.\n",
    "\n",
    "This means that we can submit to the cluster a task that takes as argument an execution promise of an instruction that has been previosly submitted. \n",
    "\n",
    "The `gather` function can thus be invoked at the end of the program, invoked only when the results have to be effectively retrived from the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = client.submit(increment, 1)\n",
    "y = client.submit(decrement, 2)\n",
    "total = client.submit(add, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total)          # This is still the execution promise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(total)  # This is the final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Parallelization of a for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, with the previous knowledge of what discussed with Spark, and the `delayed` and `compute` Dask operations, we should already be able to run simple \"dummy\" tasks.\n",
    "\n",
    "Starting from a list $\\vec{x}$ of numbers:\n",
    "1. increment each element $x_i$ by a random value $\\delta x_i$ (in the 0-1 range)\n",
    "2. multiply the resulting value by 10\n",
    "3. sum all resulting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "import random\n",
    "\n",
    "# increment function\n",
    "def add_rand(x):\n",
    "    ...\n",
    "\n",
    "# multiplication function\n",
    "def mult_ten(x):\n",
    "    ...\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "results = ...\n",
    "\n",
    "for x in data:\n",
    "    y =  ... # put your code here.\n",
    "    z =  ... # put your code here.\n",
    "    results. ... # put your code here.\n",
    "    \n",
    "total = ... #put your code here. \n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"result: \",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same task with the eager `submit` execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "results = []\n",
    "\n",
    "for x in data:\n",
    "    y =  ... # put your code here.\n",
    "    z =  ... # put your code here.\n",
    "    results. ... # put your code here.\n",
    "    \n",
    "total = ... #put your code here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"result: \",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly another alternative to the previous approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "y =  ... # put your code here.\n",
    "z =  ... # put your code here.\n",
    "    \n",
    "total = ... #put your code here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"result: \",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Sum reduction on a tree\n",
    "\n",
    "So far we acted mostly with _map_ like operations, and we collected all outputs in a single `sum` operation.\n",
    "\n",
    "We might want to briefly explore how we could write in Dask an equivalent _reduce_ function to evaluate the sum of a list of elements pair-wise\n",
    "\n",
    "Here there is a schema on wath we have to do:\n",
    "\n",
    "```\n",
    "finish           total             single output\n",
    "    ^          /        \\\n",
    "    |        c1          c2        neighbors merge\n",
    "    |       /  \\        /  \\\n",
    "    |     b1    b2    b3    b4     neighbors merge\n",
    "    ^    / \\   / \\   / \\   / \\\n",
    "start   a1 a2 a3 a4 a5 a6 a7 a8    many inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there is the usual pair-reduction algorithm with nested for loop and a bit of simple Python logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "L = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "start = time.time()\n",
    "while len(L) > 1:\n",
    "    _ = []\n",
    "    for i in range(0, len(L), 2):\n",
    "        if i+1 < len(L):\n",
    "            pair_sum = add(L[i], L[i + 1])  # add neighbor\n",
    "        else:\n",
    "            pair_sum = add(L[i], 0)         # add 0\n",
    "        _.append(pair_sum)\n",
    "    L = _ \n",
    "    \n",
    "print(\"result:\",L[0])\n",
    "end = time.time()\n",
    "\n",
    "print(\"Computation took {:.2f}s\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parallelize this reduce task, we define as `delayed` the pair-wise sum of neighbour elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "while len(L) > 1:\n",
    "   ... # put your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L[0].visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "result = L[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"result\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Count how many words are present in a series of documents \n",
    "\n",
    "Starting from a dataset of pieces of text taken from `sklearn` we would like to count how many words are present in each document and calculate how many words are in the total dataset. \n",
    "\n",
    "The documents are ~8000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import time\n",
    "\n",
    "categories = [\n",
    "     'comp.graphics',\n",
    "     'comp.os.ms-windows.misc',\n",
    "     'comp.sys.ibm.pc.hardware',\n",
    "     'comp.sys.mac.hardware',\n",
    "     'comp.windows.x',\n",
    "     'misc.forsale',\n",
    "     'rec.autos',\n",
    "     'rec.motorcycles',\n",
    "     'rec.sport.baseball',\n",
    "     'rec.sport.hockey',\n",
    "     'sci.crypt',\n",
    "     'sci.electronics',\n",
    "     'sci.med',\n",
    "     'sci.space'\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories ).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Documents in the dataset:\",len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_document(text):\n",
    "    ... # put your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single-threaded execution in Python can be run by the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "total_words_in_all_data = 0\n",
    "total_words_in_document = []\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    total_words_in_document. ... # put your code here.\n",
    "    total_words_in_all_data = ... # put your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of words in the dataset: {}\".format(total_words_in_all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(total_words_in_document,bins=range(0,1000,10));\n",
    "plt.xlabel('words per document');\n",
    "plt.ylabel('counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed version using `map` and `submit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "total_words_in_document = ...\n",
    "total_words_in_all_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ...\n",
    "print(\"Total number of words in the dataset: {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Calculate the first $n$ Fibonacci numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the algorithm to evaluate the sequence of Fibonacci up to the $n$-th element:\n",
    "\n",
    "1,1,3,5,8,13,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci_sequential(num):\n",
    "    i = 1\n",
    "    if num <= 0:\n",
    "        ... # put your code here.\n",
    "    elif num == 1:\n",
    "        ... # put your code here.\n",
    "    elif num == 2:\n",
    "        ... # put your code here.\n",
    "    elif num > 2:\n",
    "        ... # put your code here.\n",
    "        while i < (num - 1):\n",
    "            ... # put your code here.\n",
    "            i += 1\n",
    "    return fibonacci\n",
    "\n",
    "result = fibonacci_sequential(8)\n",
    "print(\"The first n fibonacci numbers are: \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the same algorithm using the Dask lazy evaluation, with `delayed`, and inspect the task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to generalize the append\n",
    "def append(arr = [], val = 0):\n",
    "    if val != None:\n",
    "        arr.append(val)\n",
    "    return arr\n",
    "\n",
    "def fibonacci(num):\n",
    "    i = 1\n",
    "    if num == 0:\n",
    "        ... # put your code here\n",
    "    elif num == 1:\n",
    "        ... # put your code here.\n",
    "    elif num == 2:\n",
    "        ... # put your code here.\n",
    "    elif num > 2:\n",
    "        ... # put your code here\n",
    "        while i < (num - 1):\n",
    "            ... # put your code here.\n",
    "            i += 1\n",
    "    return ...  # put your code here\n",
    "result = fibonacci(8)\n",
    "result.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Monte Carlo Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we want to integrate of a function via MonteCarlo technique, as you have discussed in LCP Module A.\n",
    "\n",
    "Let's use the function $$f(x) =\\sin^2{\\frac{1}{x(2-x)}}$$ and let's integrate in the range $(0,2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return (np.sin(1/(x*(2-x))))**2\n",
    "\n",
    "x=np.linspace(-0.2,2.2,1000)\n",
    "\n",
    "plt.figure(figsize=(16,6));\n",
    "plt.plot(x,f(x),'grey','.');\n",
    "plt.fill_between(x[np.where((x>0) & (x<2))],[1]*len(np.where((x>0) & (x<2))), alpha=0.2);\n",
    "plt.fill_between(x[np.where((x>0) & (x<2))],f(x[np.where((x>0) & (x<2))]), alpha=0.2);\n",
    "plt.vlines([0, 2], 0, 1, colors = [\"k\", \"k\"], linestyles = [\"dashed\", \"dashed\"],linewidths=[3,3],zorder=20);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('$f(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the single-thread Python code to execute this task, and evaluate the integral over N=100 000 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Monte Carlo integration\n",
    "N=100000\n",
    "count=[]\n",
    "def pass_function():\n",
    "    x=2*np.random.random()\n",
    "    y=np.random.random()\n",
    "    return 1 if y<f(x) else 0\n",
    "\n",
    "for i in range(N):\n",
    "    count.append(pass_function())\n",
    "\n",
    "I=2*sum(count)/N\n",
    "print(\"Integral=\",I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the minimal changes to the code to deploy the same integral computation on the Dask cluster\n",
    "\n",
    "**NOTE**: Do NOT use 100 000 points in this case, but _limit the computation to N=1000 points_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo integration\n",
    "N=1000\n",
    "\n",
    "... # put your code here\n",
    "\n",
    "real_count = ...     # put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "I=2*real_count.compute()/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Integral=\",I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the reason why we are getting worse performance with respect to the single threaded execution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: $\\pi$ via MonteCarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can re-run the evaluation of $\\pi$ from the MonteCarlo technique, as done during the Spark hands-on session.\n",
    "\n",
    "Just be careful that so far with Dask we are not creating data partitions _just yet_.\n",
    "\n",
    "We are instructing Dask to run a simple task per each entry in our list, thus results in a strong overhead.\n",
    "\n",
    "For this reason, use a very limited number of points (start with ~10 and max ~ 100), and check the status of the job from the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain python - single threaded\n",
    "%%time \n",
    "\n",
    "num_points = 10000\n",
    "\n",
    "points_in_circle = []\n",
    "\n",
    "def in_circle():\n",
    "    ## simulate the point and check if\n",
    "    ## it is inside the circle \n",
    "    ## return 0 or 1\n",
    "    ... # put your code here\n",
    "\n",
    "for p in range(num_points):\n",
    "    ... # put your code here\n",
    "\n",
    "num_points_inside = ... # put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print result\n",
    "print (\"pi =\", 4*num_points_inside/num_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `delayed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "import random\n",
    "\n",
    "num_points = 100\n",
    "\n",
    "... # put your code here\n",
    "\n",
    "points_inside = ... # put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# points_inside.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_points_inside = points_inside.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print result\n",
    "print (\"pi =\", 4*num_points_inside/num_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `submit` we have to be very careful...\n",
    "Calling naively `client.map` with a function returning a random variable would imply returning all the times the same exact random value (we have 1 function mapped on all the data).\n",
    "\n",
    "We can however instruct Dask that this function is in fact `inpure`, namely, is a function that does not really returns the same result for every execution.\n",
    "\n",
    "Thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "... # put your code here\n",
    "\n",
    "points_in_circle = client.map(... \"\"\" put your code here \"\"\", pure=False) # pure=False is needed to use the map function in this case\n",
    "\n",
    "... # put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print result\n",
    "print (\"pi =\", 4*num_points_inside/num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
