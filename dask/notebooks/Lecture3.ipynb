{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f76699",
   "metadata": {},
   "source": [
    "# Continuing on the Dask Data structures\n",
    "\n",
    "Dask offers several pythonic data structures to handle and operate with larger-than-memory data in a distributed system.\n",
    "- `dask.bag`: distributed generic python list. The Dask equivalent to a PySpark RDD\n",
    "- `dask.array`: distributed numpy arrays\n",
    "- `dask.dataframe`: distributed pandas dataframes\n",
    "\n",
    "All the high-level data structure APIs are optimized to exploit the DAG optimization features of the Dask scheduler, and thus rely on lazy computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21d093",
   "metadata": {},
   "source": [
    "## Start the Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779422b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# use the provided master\n",
    "client = Client('dask-scheduler:8786')\n",
    "    \n",
    "# print the status of the client        \n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08409bcd",
   "metadata": {},
   "source": [
    "## Dask DataFrame\n",
    "\n",
    "The `dask.dataframe` API implements a blocked parallel DataFrame object that mimics a large subset of the Pandas DataFrame interfaces.\n",
    "\n",
    "A Dask DataFrame is comprised of many in-memory Pandas DataFrames or Series hosted on all machines of the cluster. A Dask DataFrame has its table separated (partitioned) along the index **\\*** .\n",
    "\n",
    "One operation on a Dask DataFrame triggers many Pandas operations on the constituent Pandas DataFrames in a way that is mindful of potential parallelism and memory constraints.\n",
    "\n",
    "**\\*** _Quick question: What kind of data partitioning are we referring to in this case? Vertical or Horizontal?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025416a8",
   "metadata": {},
   "source": [
    "### (On Data Partitioning)\n",
    "\n",
    "Partitioning is one of the key aspects of data management and processing in distributed systems, including frameworks like Spark and Dask.\n",
    "\n",
    "Distributing our dataset into a large number of small partitions enables parallel processing on each node. However, it's important to note that every call the central scheduler makes to access data on a remote partition (on a worker) incurs some time, typically in the order of a few hundred milliseconds.\n",
    "\n",
    "As users of distributed processing systems, it is our responsibility (not Dask's or Spark's) to decide and optimize the number of partitions. The distributed framework may make an initial choice or guess, but optimizing performance depends on us making the right choice based on the task at hand.\n",
    "\n",
    "For example, when initially loading data from a set of files, the number of partitions might be equal to the number of CSV files we are importing. However, as we modify the size of our DataFrames through filtering or joining, it may be wise to reconsider the number of partitions to optimize the Dask scheduler's overhead.\n",
    "\n",
    "_There is always a cost associated with having too many or too few partitions, and unfortunately, there is no single rule to determine the \"right\" number of partitions._\n",
    "\n",
    "As a _rule of thumb_, partitions should fit comfortably in memory. However, they should not be too numerous, as the overhead of the central scheduler can impact computation performance.\n",
    "\n",
    "This means that the number of partitions should be in a \"reasonable range,\" and their number and size should be chosen by the user to optimize execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab6863",
   "metadata": {},
   "source": [
    "## Basic operations on DataFrames\n",
    "\n",
    "Let's start by reading a set of structured files (comma-separated) into a DataFrame.\n",
    "\n",
    "In pure Pandas, we would have to loop over all the files, open each one of them as a DataFrame, and concatenate all the DataFrames into a large DataFrame.\n",
    "\n",
    "The resulting DataFrame is a single entity stored in memory, allowing for single-threaded data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67799e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls datasets/accounts_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -10 datasets/accounts_csv/accounts.0.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "from glob import iglob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# define the path to the CSV files\n",
    "path = os.path.join('datasets', 'accounts_csv', 'accounts.*.csv')\n",
    "\n",
    "# use glob to find all matching file paths\n",
    "all_files = iglob(path, recursive=True)\n",
    "\n",
    "# create a generator to read each CSV file as a Pandas DataFrame\n",
    "dataframes = (pd.read_csv(f) for f in all_files)\n",
    "\n",
    "# concatenate all DataFrames into a single large DataFrame\n",
    "large_dataframe = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bcad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the pandas DataFrame\n",
    "large_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8aa2f9",
   "metadata": {},
   "source": [
    "In Dask, DataFrames can be created from a glob pattern using the wildcard `*`, which reads all files in the specified path matching that pattern into the same Dask DataFrame.\n",
    "\n",
    "It's important to remember that Dask DataFrames are a collection of Pandas DataFrames scattered across the workers.\n",
    "\n",
    "When reading data into a Dask DataFrame, Dask automatically creates parallel jobs to read the data in parallel. By default, it creates one parallel job per chunk, which is typically the number of input files. However, it is recommended to consider how we want to partition our dataset to make the best use of our workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e80a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# define the path to the CSV files\n",
    "path = os.path.join('datasets', 'accounts_csv', 'accounts.*.csv')\n",
    "\n",
    "# read the CSV files into a Dask DataFrame\n",
    "df = dd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d77131",
   "metadata": {},
   "source": [
    "As always, reading data is a lazy operation, meaning it is postponed until we have something to compute.\n",
    "\n",
    "Let's compute the length of the entire DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7fe7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the length of the Dask DataFrame\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda64378",
   "metadata": {},
   "source": [
    "At this stage, each file was loaded into a separate Pandas DataFrame and scattered across the nodes.\n",
    "\n",
    "Computing `len()` means that `len` was applied to each individual Pandas DataFrame, and then the sub-results were aggregated and combined to provide the overall total length of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713351ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"print\" the Dask DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813a31c",
   "metadata": {},
   "source": [
    "We can reassign the number of partitions to the Dask DataFrame object using the `repartition` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef75a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the number of partitions to 8\n",
    "df = df.repartition(npartitions=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"print\" the Dask DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the length of the Dask DataFrame\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of partitions of the Dask DataFrame\n",
    "df.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5703b7",
   "metadata": {},
   "source": [
    "It is worth mentioning that unlike Pandas, Dask only reads a sample from the beginning of the file (or the list of files) to start inferring the data types.\n",
    "\n",
    "These inferred data types are then enforced when reading all partitions, which can potentially lead to incorrect data type assignments.\n",
    "\n",
    "For example, consider the case where we start reading a CSV file in which the first $n$ entries of a column are all integers, but the column is actually supposed to be of type _string_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV files - explicitely parse dates \n",
    "df = dd.read_csv(os.path.join('datasets', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70641981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the DataFrame dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37398b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the head of the *distributed* Dask DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed6418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tail of the *distributed* Dask DataFrame\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bddaa4",
   "metadata": {},
   "source": [
    "In this case, the datatypes inferred for `CRSElapsedTime` and `TailNum` are in fact incorrect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3872d18",
   "metadata": {},
   "source": [
    "However, we can force Dask to interpret the data types as we want by using the `dtype` assignment. This is usually the most viable and robust solution, but not the only one.\n",
    "\n",
    "Alternatively, we could:\n",
    "- Increase the size of the sample used to infer the data types.\n",
    "- Use `assume_missing` to make Dask assume that columns inferred to be `int` (which don't allow missing values) are actually `floats` (which do allow missing values).\n",
    "\n",
    "However, in general, the best solution is still to hard-code the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f5e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV files with parsing dates and specifying data types\n",
    "df = dd.read_csv(os.path.join('datasets', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3dca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition the dataframe\n",
    "df = df.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d8d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the tail of the dataframe\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec163a79",
   "metadata": {},
   "source": [
    "While on the topic of reading data from files, it is worth mentioning that Dask offers a wide range of APIs for importing structured data from various file formats into a DataFrame object, including:\n",
    "- CSV\n",
    "- JSON\n",
    "- Avro\n",
    "- Parquet\n",
    "\n",
    "For a more extensive list, please refer to the official documentation at the following [link](https://docs.dask.org/en/stable/dataframe-create.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b7b2f",
   "metadata": {},
   "source": [
    "## Computations with Dask DataFrames\n",
    "\n",
    "Dask DataFrames are designed to closely mimic the Pandas DataFrame API, with nearly a 1-to-1 correspondence. Dask provides a wrapper around the Pandas API to manage the same calls on the distributed collection of Pandas DataFrames, which in our case are the partitions of our dataset.\n",
    "\n",
    "Using Dask DataFrames is beneficial when Pandas reaches its limits, such as when dealing with datasets larger than memory or tasks that can be efficiently parallelized.\n",
    "\n",
    "However, when working with small datasets or tasks that are not easily parallelizable, Pandas will always be the better choice.\n",
    "\n",
    "For this reason, the typical computing pattern when working with Dask DataFrames is as follows:\n",
    "1. Use `Dask Bag` to ingest data from semi/unstructured sources and preprocess it into Dask DataFrames.\n",
    "2. Use `Dask DataFrame` for parallel computations and data reduction to produce a slimmed-down DataFrame that can fit in memory. Then, convert it into a single Pandas DataFrame.\n",
    "3. Use `Pandas` for simple (yet fast) DataFrame operations for non-parallelizable tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ba854",
   "metadata": {},
   "source": [
    "We can always retrieve data from all partitions and create a Pandas DataFrame from a Dask DataFrame using the `compute` method on the DataFrame itself.\n",
    "\n",
    "However, it's important to note that this operation should be done carefully and only at the appropriate time, especially when dealing with very large (possibly terabytes-sized) Dask DataFrames. It should only be performed when we have reduced the DataFrame to a manageable size that can fit into our computer's memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe13aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Dask DataFrame to Pandas DataFrame\n",
    "pandas_df = df.compute()\n",
    "\n",
    "# display the Pandas DataFrame\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900aa585",
   "metadata": {},
   "source": [
    "Let's work with basic DataFrame operations and compare their impact in Dask vs Pandas.\n",
    "\n",
    "In Dask, operations on columns and individual items are easily parallelized, making them very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the max of the DepDelay column\n",
    "df.DepDelay.max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30accc15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize the graph for this operation\n",
    "df.DepDelay.max().visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the average arrival delay (ArrDelay) \n",
    "# of all flights with carrier UnitedAirlines (UA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph for this last operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c727a39",
   "metadata": {},
   "source": [
    "Dask DataFrames also provide efficient implementations of operations that involve minimal shuffling, such as group-based aggregations and merge operations.\n",
    "\n",
    "This means that operations like `groupby`, `resample`, `rolling`, and similar operations are still _reasonably_ fast, thanks to extensive under-the-hood optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d05a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max airtime by combinations of origin and destination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph for this last operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bb15e",
   "metadata": {},
   "source": [
    "As we can see from the graph, the `groupby` method returns a single object from the computation, stored in a single partition. This is generally a reasonable choice, as the result of a groupby aggregation is typically small enough to fit into a single worker's memory, eliminating the need to split the result into multiple partitions.\n",
    "\n",
    "However, there may be cases where the returned object is very large, depending on the input datasets. In such situations, we can optimize the number of output partitions by using the `split_out` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c55d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max airtime by combinations of origin and destination\n",
    "# split the result of the groupby operation in 4 partitions\n",
    "df.groupby(['Origin','Dest']).AirTime.max(split_out=4).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6b263",
   "metadata": {},
   "source": [
    "Note: Dask also supports the same aggregate syntax as Pandas, allowing us to run several aggregations simultaneously on the same group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5b200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# group the DataFrame by 'Origin' AND 'Dest', and calculate the mean AND standard deviation of 'AirTime'\n",
    "df.groupby(['Origin', 'Dest'])['AirTime'].aggregate(['mean', 'std']).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0bacc",
   "metadata": {},
   "source": [
    "There are **certain operations** for which the **mapping with the standard Pandas APIs does not hold** in Dask.\n",
    "\n",
    "For example, slicing and feature-based indexing using `loc` work as expected in Dask, but the position-based indexing operator `iloc` does not behave the same way as it does in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows where 'Dest' column equals 'DEN' and retrieve the 'UniqueCarrier' column\n",
    "df.loc[df['Dest'] == 'DEN', ['UniqueCarrier']].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91016aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph for this last operation\n",
    "df.loc[df['Dest']=='DEN',['UniqueCarrier']].visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae38193",
   "metadata": {},
   "source": [
    "In Dask, using `iloc` for position-based indexing will raise an exception instead of behaving as it does in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use iloc to locate the first i-th rows of the *distributed* DataFrame\n",
    "df.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33391950",
   "metadata": {},
   "source": [
    "This behavior occurs because Dask DataFrame does not consider the length of partitions or the row ordering within each partition. When records are sharded across multiple nodes, determining the true ordering of rows becomes ambiguous.\n",
    "\n",
    "While you can still use `iloc` to select the index of a specific column for all rows in all partitions, such as `iloc[:, 0:10]`, it is important to note that `iloc` can be extremely inefficient in Dask due to the underlying data distribution and lack of explicit row ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3ad8f",
   "metadata": {},
   "source": [
    "Other operations that are **inefficient** in Dask include actions that require _re-indexing_ of the entire DataFrame, _sorting_, or _merging based on a column that is not the DataFrame's index_.\n",
    "\n",
    "Dask optimizes the data distribution by utilizing the range of the index column to subdivide the data into partitions. However, setting an index on a DataFrame requires sorting the entire dataset by the specified column, which can be an expensive process. While sorting can be slow, it can be beneficial to perform it (although infrequently) to accelerate subsequent computations.\n",
    "\n",
    "Dask also leverages the index in merge/join operations. Performing a join on a column that is not the DataFrame's index is also an expensive operation.\n",
    "\n",
    "After performing an unavoidable and expensive operation such as reshuffling the data, you can persist the new DataFrame to speed up subsequent computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e7189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select rows where 'UniqueCarrier' is 'UA' and 'Dest' is 'DEN', and set the index to 'Date'\n",
    "df_reindexed = df[(df.UniqueCarrier=='UA') & (df['Dest']=='DEN')].set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b95f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute a 5-days rolling average of the related ArrivalDelay\n",
    "df_reindexed.ArrDelay.rolling('5D').mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f34bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the graph for this last operation\n",
    "df_reindexed.ArrDelay.rolling('5D').mean().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350421a",
   "metadata": {},
   "source": [
    "## Shared, reapeated and intermediate computations\n",
    "\n",
    "When performing the computations mentioned above, there may be cases where the same operation needs to be repeated multiple times. \n",
    "\n",
    "In Dask DataFrame, for most operations, the arguments are hashed, allowing for duplicate computations to be shared and computed only once.\n",
    "\n",
    "For example, let's consider computing the mean and standard deviation of departure delay for all non-canceled flights. Since Dask operations are lazy, the computed values are not the final results yet. They represent the recipe required to obtain the final result.\n",
    "\n",
    "If we change the approach and compute these values with two individual calls to `compute`, there will be no sharing of intermediate computations, resulting in an overall speedup of the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd69d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache the distributed DataFrame in the memory of the workers\n",
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7675705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the head of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "# create non-cancelled dataframe (lazy)\n",
    "non_cancelled = df[df.Cancelled==0]\n",
    "\n",
    "# create mean DepDelay series (lazy)\n",
    "mean_delay    = non_cancelled.DepDelay.mean()\n",
    "\n",
    "# create std DepDelay series (lazy)\n",
    "std_delay     = non_cancelled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18710159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# compute the time to run both functions\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a2fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# compute the time to combining both functions into a single compute call\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7035b20",
   "metadata": {},
   "source": [
    "Using `dask.compute` takes approximately half the time compared to the other approach. This is because when calling `dask.compute`, the task graphs for both results are merged, allowing shared operations to be performed only once instead of twice. Specifically, `dask.compute` performs the following operations once:\n",
    "\n",
    "- Calls to the `read_csv` function\n",
    "- Filter on cancelled flights\n",
    "- Some necessary reductions (such as sum and count)\n",
    "\n",
    "Let's take a look at the computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99272d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph and spot the two outputs\n",
    "dask.visualize(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247468d7",
   "metadata": {},
   "source": [
    "What we can observe from the computation graph is that many operations have been merged and used only once, resulting in an optimization of the overall computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99b0ee",
   "metadata": {},
   "source": [
    "## Re-run the same exercise from the Spark notebook using Dask\n",
    "\n",
    "To apply all we discussed about Dask high- and low-level APIs, we can re-run the same task we used as an example analysis pipeline in PySpark for the DiMuon invariant mass of the LHC collision data.\n",
    "\n",
    "Starting from the JSON files already used in the previous PySpark example, we will need to:\n",
    "1. Load all files using the Dask Bag API.\n",
    "2. Parse the JSON event collections into Python dictionaries.\n",
    "3. Use a `foldby` operation to count the number of events per sample (mc and data).\n",
    "4. Plot the distribution of the number of muons in the MC and data samples from the Dask Bag object.\n",
    "5. Select only events with exactly 2 muons with opposite charge and fill a Dask DataFrame with the normalized results.\n",
    "6. Create the transverse momenta $p_{T,1}$ and $p_{T,2}$ features and create a 2D plot of these features.\n",
    "$$p_T = \\sqrt{p_x^2 + p_y^2}$$\n",
    "7. Retain only the dimuon pairs where both muons' $p_T$ is greater than 15 (GeV).\n",
    "8. Create the components of the dimuon 4-momentum\n",
    "$$(E,P_x,P_y,P_z) = (E_1+E_2,p_{x,1}+p_{x,2},p_{y,1}+p_{y,2},p_{z,1}+p_{z,2})$$\n",
    "9. create and plot the invariant mass spectrum\n",
    "$$M = \\sqrt{E^2 - (P_x^2 + P_y^2 + P_z^2) }$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fd024",
   "metadata": {},
   "source": [
    "Before starting, from a terminal (external to the docker container you are working in), copy the `MAPD-B/spark/datasets/lecture2/dimuon` folder into the `MAPD-B/dask/notebooks/datasets/.` path, or create a symbolic link to that path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37cfdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart the client to free up the workers' memory\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1870e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the dimuon json files in a bag, each containing the text lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02f508",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# extract the json structure from the lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2fad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of events per each sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the number of muons in the mc and data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03589adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only events with exactly 2 muons with opposite charge and fill a dask dataframe with the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b476bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new pT1 and pT2 features \n",
    "# pT = sqrt(px^2 + py^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 2 dimensional distribution of the dimuon pair pTs in the mc sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only those dimuon pairs where both muons' pT is greater than 15 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fdb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 4-momentum features \n",
    "\n",
    "# and compute the invariant mass of the dimuon pair\n",
    "# M = sqrt(E*E - (Px*Px + Py*Py + Pz*Pz))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the slimmed down dataframe containing only the sample, Mass, and pT of the objects\n",
    "# and store it in a Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da766d65",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the distribution of the dimuon invariant mass in the mc and data sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a7907",
   "metadata": {},
   "source": [
    "### (in the case of memory management issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185e7d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to reclaim some memory from the workers\n",
    "import ctypes\n",
    "\n",
    "# invoke the C library libc.so.6 to deallocate memory from the worker\n",
    "def trim_memory() -> int:\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)\n",
    "\n",
    "# run the `trim_memory` function on all Dask workers to reclaim memory\n",
    "client.run(trim_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d376528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart the client (this will reset all operations done so far)\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ecdf8c",
   "metadata": {},
   "source": [
    "## Stop client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6189dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19c2f9",
   "metadata": {},
   "source": [
    "Finally, use `docker compose down` to stop and clear all running containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c79501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
