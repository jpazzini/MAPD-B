{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f76699",
   "metadata": {},
   "source": [
    "# Dask Data structures\n",
    "\n",
    "Dask offers several pythonic data structures to handle and operate with larger-than-memory data in a distributed system.\n",
    "- `dask.bag`: distributed generic python list. The Dask equivalent to a PySpark RDD\n",
    "- `dask.array`: distributed numpy arrays\n",
    "- `dask.dataframe`: distributed pandas dataframes\n",
    "\n",
    "All the high-level data structure APIs are optimized to exploit the DAG optimization features of the Dask scheduler, and thus rely on lazy computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21d093",
   "metadata": {},
   "source": [
    "## Start the Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "\n",
    "# -> 'local'\n",
    "# -> 'docker_container'\n",
    "# -> 'docker_cluster'\n",
    "\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CLUSTER_TYPE $CLUSTER_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out script_out\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    echo \"Launching scheduler and worker\"\n",
    "    \n",
    "    HOSTIP=`hostname -I | xargs`\n",
    "    \n",
    "    echo \"dask-scheduler --host $HOSTIP --dashboard-address $HOSTIP:8787\"\n",
    "    \n",
    "    # dask scheduler \n",
    "    dask-scheduler --host $HOSTIP --dashboard-address $HOSTIP:8787 &\n",
    "\n",
    "    # dask worker\n",
    "    dask-worker $HOSTIP:8786 --memory-limit 2GB --nworkers 2 &\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ip = !hostname -I | xargs\n",
    "host_ip = host_ip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf23cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "if CLUSTER_TYPE == 'local':\n",
    "    \n",
    "    client = Client()\n",
    "\n",
    "elif CLUSTER_TYPE == 'docker_container':\n",
    "    \n",
    "    client = Client('{}:8786'.format(host_ip))\n",
    "    \n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # use the provided master\n",
    "    client = Client('dask-scheduler:8786')\n",
    "    \n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08409bcd",
   "metadata": {},
   "source": [
    "## Dask Bag\n",
    "\n",
    "Bags are very powerful and flexible data structures.\n",
    "The Dask Bag offers essentially the same degree of flexibility as the RDD in pySpark.\n",
    "They are parallelized general collections of objects, like Python’s built-in `list`, and can therefore hold any Python objects, whether they are custom classes or built-in types. \n",
    "This makes it possible to contain very complicated data structures, like raw text or nested JSON data, and navigate them with ease.\n",
    "\n",
    "For these reasons, Dask bags are often used to parallelize simple computations on unstructured or semi-structured data like text data, log files, JSON records, or user defined Python objects, using MapReduce-like approaches to load/inspect/filter arbitrary datasets (structured or unstructured).\n",
    "\n",
    "Dask Bag implements in fact operations like `map`, `filter`, `groupby` and aggregations on collections of Python objects.\n",
    "It does this in parallel using Python iterators similarly to a parallel version of itertools.\n",
    "\n",
    "Once a first stage of data-preparation is completed using Dask Bag, it is quite common to reduce and convert the data into more suitable data structures, such as Dask Arrays or Dask Dataframes, which will be covered later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd63846",
   "metadata": {},
   "source": [
    "### Create and Take from a Bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced8b7d",
   "metadata": {},
   "source": [
    "We can create a `Bag` from a Python sequence, from files, from data on cloud-storage such as Amazon AWS S3, etc.\n",
    "For a comprehensive overview on the ways to access remote data from DFS, S3, and others, do refer to the official documentation at the [link](https://docs.dask.org/en/stable/how-to/connect-to-remote-data.html).\n",
    "\n",
    "We can as well create a Bag from a function we have declared as `delayed`.\n",
    "This way, we can generate data from a distributed application, and then access the data with the bag API before computing a result.\n",
    "\n",
    "The data are partitioned into blocks, usually with multiple items per block, depending on the datasets, the cluster resources, and our choice with the parameter `npartitions`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba350e",
   "metadata": {},
   "source": [
    "Let's start by creating some simple data from a python list.\n",
    "Clearly, as python is a dinamically typed language, this can be a simple array of integers, or an arbitrary collection fo multiple data types (numbers, strings, objects, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "# each element is an integer\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "b = db.from_sequence(data, npartitions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905079b",
   "metadata": {},
   "source": [
    "As previosly mentioned, Dask data structures embody the lazy programming paradigm.\n",
    "The data is thus not yet stored on the cluster, as we have not acted with an operation such as `compute`.\n",
    "\n",
    "In general, we don't want to return the entire data stored on the cluster, but we might want to inspect few elements.\n",
    "We can do that with `take(n_elements)`.\n",
    "The returned data will be a tuple containing the first n_elements of the Bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49904962",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266bd9c",
   "metadata": {},
   "source": [
    "Data can be extracted from text files by providing the list of all files, or with the `*` wildcard.\n",
    "\n",
    "By default, the resulting bag will have one item per line and one file per partition (so be careful when partitioning the data).\n",
    "\n",
    "A nice feature of reading text files Dask is that it handles standard compression libraries (like gzip, bz2, xz) automatically as they will be inferred by the file name extension, or by using the compression='gzip' keyword.\n",
    "\n",
    "For instance, we can load up in a Bag a number of files from a local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d025db",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls datasets/accounts_json/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3f608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "b = db.read_text(os.path.join('datasets','accounts_json','accounts.*.json.gz'),\n",
    "                 files_per_partition=4)\n",
    "example = b.take(1)\n",
    "\n",
    "print(type(example))\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9554e",
   "metadata": {},
   "source": [
    "`Bag` objects hold the standard functional APIs including `map`, `filter`, `groupby`, etc..\n",
    "\n",
    "Operations on `Bag` objects create new bags, thus we can daisy-chain multiple operations together to manipulate the data until we reach the desired result.  \n",
    "\n",
    "We can finally call the `.compute()` method to trigger execution, as we saw for any `delayed` object.  \n",
    "\n",
    "As a bag is always a delayed object in nature, there is no real need to specify that the functions we want to apply to the dataset are further delayed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_even(n):\n",
    "    return n % 2 == 0\n",
    "\n",
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "c = b.filter(is_even).map(lambda x: x ** 2)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbc743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], npartitions=3)\n",
    "c = b.filter(is_even).map(lambda x: x ** 2)\n",
    "c.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c66468",
   "metadata": {},
   "source": [
    "### Example: Open and preprocess JSON data\n",
    "\n",
    "We'll start from the dummy dataset of gzipped JSON data in your data directory.\n",
    "This is a dataset analogous to what you might collect off of a document store database (e.g. Mongo) or by scaping a website using the dedicated API.\n",
    "\n",
    "Each line of each documnent is a JSON encoded dictionary with the following keys\n",
    "\n",
    "*  `id`: Unique identifier of the customer\n",
    "*  `name`: Name of the customer\n",
    "*  `transactions`: a list of key-value pairs in the form of `transaction-id` and `amount` pairs (one for each transaction for the customer in that file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07021218",
   "metadata": {},
   "source": [
    "1. Create a Bag reading out the dataset from the text files \n",
    "2. Map the `json.loads` function on each message to extract the data in the form of python dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create a dask bag from the files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. read the data from the json format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9df035",
   "metadata": {},
   "source": [
    "Once the JSON data is mapped into the proper Python objects (dictionaries, lists, etc.) we can perform dedicated opeartions by creating small Python functions to run on our data.\n",
    "\n",
    "The most basic operations we can perform on a Dask Bag are the following:\n",
    "- `map`: apply a function to each element\n",
    "- `filter`: retain only the elements passing a given function\n",
    "- `pluck`: select a specific nested field, as from a python dictionary `element[field]`\n",
    "- `flatten`: un-fold the dictionary into a list-like object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fc7fe",
   "metadata": {},
   "source": [
    "1. compute the average number of transactions for each entry of a user named \"Alice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f076cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "# AND count the total number of transactions for each entry in the dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "# AND count the total number of transactions (as 'count') for each entry in the dataset \n",
    "# AND return only the 'count' values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f06cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the records from users named \"Alice\"\n",
    "# AND count the total number of transactions (as 'count') for each entry in the dataset \n",
    "# AND return only the 'count' values\n",
    "# AND compute the average of the counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e9f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph of the tasks composing the job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc0880",
   "metadata": {},
   "source": [
    "2. compute the average amount for all transactions for all users named \"Alice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the relevant transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333441ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the relevant transactions\n",
    "# AND return only the \"amount\" in a bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d440031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the relevant transactions\n",
    "# AND return only the \"amount\" in a bag\n",
    "# AND compute the average of all transactions amounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph of the tasks composing the job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb1002",
   "metadata": {},
   "source": [
    "Additional standard operations on Dask Bags can be performed by mean of groupby and aggregation functions.\n",
    "\n",
    "-  `groupby`:  Shuffles data so that all items with the same key are in the same key-value pair\n",
    "-  `foldby`:  Walks through the data accumulating a result per key. It works as a combined groupby and reduce operation, and it allows for efficient parallel split-apply-combine tasks.\n",
    "\n",
    "As always, we must remember that any \"data-shuffle heavy\" operation (such as `groupby`) are very expensive as they require to move the data across the workers.\n",
    "The `foldby` method in Dask is more complex to use but also much \"cheaper\" in terms of the computational time required, so it should be preferred whenever possibile to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461dc7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_data = ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith', 'Frank']\n",
    "\n",
    "# create a bag from the list of names\n",
    "b = db.from_sequence(names_data)\n",
    "\n",
    "# group names by length\n",
    "res = b.groupby(len) \n",
    "\n",
    "# visualize this \"simple\" graph\n",
    "res.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f646a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a261a",
   "metadata": {},
   "source": [
    "Notice how the result of the groupby operation is a tuple.\n",
    "If we need to apply functions on the elements of the tuples we can use `starmap`.\n",
    "\n",
    "The `starmap` function in Dask allows to apply a function using argument tuples, similarly to what the standard `itertools.starmap` does in python.\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple bag from a list of integers\n",
    "b = db.from_sequence(list(range(10)))\n",
    "\n",
    "# groupby even/odd numbers\n",
    "b.groupby(lambda x: x % 2).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the max of all elements in each group\n",
    "b.groupby(lambda x: x % 2)\\\n",
    " .starmap(lambda k, v: (k, max(v)))\\\n",
    " .compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the sum of the elements in each group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the graph once again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9a221",
   "metadata": {},
   "source": [
    "Foldby can be quite odd at first.  It is similar to the following functions from other libraries:\n",
    "\n",
    "*  [`toolz.reduceby`](http://toolz.readthedocs.io/en/latest/streaming-analytics.html#streaming-split-apply-combine)\n",
    "*  [`pyspark.RDD.combineByKey`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.combineByKey.html)\n",
    "\n",
    "When using `foldby` you must provide:\n",
    "\n",
    "1.  A key function on which to group elements (so far, a groupby equivalent)\n",
    "2.  A binary operator such as you would pass to `reduce` that you use to perform reduction per each group\n",
    "3.  A combine binary operator that can combine the results of two `reduce` calls on different parts of your dataset.\n",
    "\n",
    "In the Dask documentation this is summarized by stating that a `foldby` call such as this:\n",
    "```python\n",
    "dask_bag.foldby(key, binop, init)\n",
    "```\n",
    "\n",
    "Will be equivalent to a combination of two operations: a groupby and a reduce:\n",
    "\n",
    "```python\n",
    "def reduction(group):                               \n",
    "    return reduce(binop, group, init)               \n",
    "\n",
    "dask_bag.groupby(key).map(lambda (k, v): (k, reduction(v)))\n",
    "```\n",
    "\n",
    "The reduction operation must be associative. It will happen in parallel in each of the partitions of the dataset. Then all of these intermediate results will be combined by the `combine` binary operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bca96a",
   "metadata": {},
   "source": [
    "Let's re-write the equivalent group-by + starmap operation with a foldby call\n",
    "\n",
    "```python\n",
    "b.groupby(lambda x: x % 2).starmap(lambda k, v: (k, sum(v))).compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple bag from a list of integers\n",
    "b = db.from_sequence(list(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eaf754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby even/odd numbers with a foldby and find the total sum per group\n",
    "#\n",
    "#   write down a binary filter function to select only even or odd numbers\n",
    "#   write down a reduce-like operation to sum all elements\n",
    "is_even = lambda x: x % 2 == 0\n",
    "add = lambda x, y: x + y\n",
    "b.foldby(is_even, add, initial=0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the graph and compare it with the groupby implementation\n",
    "# `split_every` defaults to 8\n",
    "b.foldby(is_even, add, split_every=8).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5dc49",
   "metadata": {},
   "source": [
    "### Example with account data\n",
    "\n",
    "Take a moment to look to the `foldby` API at the [link](https://docs.dask.org/en/latest/generated/dask.bag.Bag.foldby.html#dask.bag.Bag.foldby).\n",
    "\n",
    "- Get the total number of users with the same name from the account dataset\n",
    "  1. Use a `groupby` function and measure the required computational time\n",
    "  2. Use a `foldby` function and measure the required computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b803d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group-by implementation\n",
    "%%time\n",
    "result_groupby = \"\"\"your code here\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b64db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold-by implementation\n",
    "%%time\n",
    "from operator import add\n",
    "\n",
    "def incr(tot, _):\n",
    "    \"\"\"your code here\"\"\"\n",
    "\n",
    "result_foldby = \"\"\"your code here\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd7bee",
   "metadata": {},
   "source": [
    "Compute total transfers amount per each name using a `foldby`\n",
    "\n",
    "We can proceed in two steps:\n",
    "1.  Create a function that given the input dictionary\n",
    "\n",
    "        {'name': 'Alice', 'transactions': [{'amount': 1, 'id': 123}, {'amount': 2, 'id': 456}]}\n",
    "        \n",
    "    produces the sum of the amounts, e.g. `3` in this case\n",
    "    \n",
    "2.  Modify the binary operator of the `foldby` example above so that the binary operator doesn't count the number of entries, but instead accumulates the sum of the transferred amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ab46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold-by implementation (do not compute the result just yet)\n",
    "result_foldby = \"\"\"your code here\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef61681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the foldby operation\n",
    "result_foldby.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and time the execution\n",
    "%%time\n",
    "result_foldby = result_foldby.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19245641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the result\n",
    "print(sorted(result_foldby))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a302c",
   "metadata": {},
   "source": [
    "## From Bag to pre-processed output datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81c10ab",
   "metadata": {},
   "source": [
    "Dask Bags are often used as an \"entry-point\" to ingest, decode and pre-process data, before either storing the results as an intermediated dataset (thus ready for further processing), or to flatten and structure the dataset to start using the Dask Dataframe API.\n",
    "\n",
    "Dask offers a number of methods to convert Bags into output data objects such as text files, JSON files, and more.\n",
    "Have a look at the documentation at the [link](https://docs.dask.org/en/stable/bag-creation.html#store-dask-bags) to see how these methods store the Dask Bag:\n",
    "- `to_textfiles`\n",
    "- `to_avro`\n",
    "- `to_delayed`\n",
    "\n",
    "By far the most widely used approach in data pre-processing using Dask Bags is to `extract` some raw data from the original input source, `transform` it applying some funcions to filter/reduce/create features from the original (usually messy) dataset, and finally `load` the clensed dataset into either a DataBase or a further data processing pipeline based on *structured* data.\n",
    "\n",
    "Converting a Dask Bag to a Dask Dataframe is thus a very common operation (very similar to the conversion from RDD to a Spark DataFrame).\n",
    "\n",
    "In order to convert arbitrary data into a stuctured table view, we need to flatten and normalize the dataset before invoking the `to_dataframe` Dask Bag function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b9cdd",
   "metadata": {},
   "source": [
    "As a purely illustrative example, our account data is deeply nested and not suitable for being transformed into a table-like dataframe structure.\n",
    "\n",
    "Assuming we may want to retain only the first transaction per customer, we can flatten the dataset by mapping a dedicated function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4964a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( db_js.take(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_flatten(record):\n",
    "    return {\n",
    "        'id': record['id'],\n",
    "        'name': record['name'],\n",
    "        'first_transaction_id': record['transactions'][0]['transaction-id'],\n",
    "        'first_transaction_amount': record['transactions'][0]['amount']\n",
    "    }\n",
    "\n",
    "db_js.map(dummy_flatten).take(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = db_js.map(dummy_flatten).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70535f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
