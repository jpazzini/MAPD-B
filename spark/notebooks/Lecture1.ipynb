{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Introduction to PySpark\n",
    "\n",
    "Immagine you are interested in working on a large dataset, at the scale of Petabytes:\n",
    "\n",
    "* Data are distributed in many nodes, collecting all of them in one node would be impossible\n",
    "\n",
    "* You want to first load data and explore them\n",
    "\n",
    "* Before moving to machine learning you need to preprocess data\n",
    "\n",
    "What you would like to do is to run the pre-processing on each node of the cluster where the data is distributed, and return only some high level quantities that will be used in the analysis. \n",
    "\n",
    "This is the core concept of Hadoop and Spark: the code goes to the data, rather than the data coming to the machine where the code resides (data locality). \n",
    "\n",
    "\n",
    "# Standalone Cluster\n",
    "\n",
    "For this set of lectures we will work with a standalone cluster deployed on your machine. You can use it to develop and test your application before deploying it on a large cluster.\n",
    "\n",
    "Youc can choose to:\n",
    "* **Local cluster**: Create a local cluster by downloading and configuring all components by hand\n",
    "* **Single Docker container**: Use the provided single Docker image to do the same\n",
    "* **Docker cluster**: Create the cluster with multiple Docker containers using docker-compose\n",
    "\n",
    "Run the setup only for the version you are interested and set the following variable.\n",
    "Note that if you want to use Docker you need to be running this notebook from inside a container, \n",
    "following the instructions reported in the README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "\n",
    "# -> 'local'\n",
    "# -> 'docker_container'\n",
    "# -> 'docker_cluster'\n",
    "\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Cluster\n",
    "\n",
    "<center><img src='imgs/lecture1/standalone_cluster.png'/></center>\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* Download [spark](https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz) \n",
    "\n",
    "* Extract the Spark tarball with `tar xvf spark-3.2.1-bin-hadoop3.2.tgz`\n",
    "\n",
    "* Spark 3 should run on both Java 8/11, so make sure that you have one of them as default (or `JAVA_HOME` environment variable pointing to your java installation)\n",
    "\n",
    "* If this is set correctly, typing `java --version` on your terminal shoud display something like `openjdk 11.0.15`\n",
    "\n",
    "### Create a standalone cluster\n",
    "\n",
    "We will now create a Spark standalone cluster on our local machine. More on this can be found in the [spark documentation](https://spark.apache.org/docs/latest/spark-standalone.html). \n",
    "\n",
    "First we need to start the master node.\n",
    "To do this, move into the spark directory, i.e. `cd spark-3.2.1-bin-hadoop3.2`. \n",
    "From here run the following command \n",
    "```\n",
    "./sbin/start-master.sh --host localhost --port 7077 --webui-port 8080\n",
    "```\n",
    "This will spin up the Spark master with address `spark://localhost:7077` and a cluster dashboark at `localhost:8080`.\n",
    "\n",
    "We can now create a worker with 2 cores and 2GB of memory using the following command:\n",
    "\n",
    "```\n",
    "./sbin/start-worker.sh spark://localhost:7077 --cores 2 --memory 2g\n",
    "```\n",
    "\n",
    "After a few seconds it should appear in the master’s web UI (`localhost:8080` on your browser). \n",
    "\n",
    "**Note**: in this configuration there can be at most one worker per each node, i.e. you can't start two workers. \n",
    "\n",
    "\n",
    "### Running applications\n",
    "\n",
    "We can now submit our application to the cluster with commands such as `spark-submit`. For these sessions we will use jupyter-notebooks to explore pyspark interactively. To do this we need one last package wich will set the environment variables needed to get pyspark modules and use the correct version of python. \n",
    "\n",
    "The package can be installed via pip running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize all the required variables with `findspark.init()` by passing the path to the spark folder we downloaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init('/home/pazzini/work/courses/MAPD_B/MAPD-B/spark/spark-3.2.1-bin-hadoop3.2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some of the env variables\n",
    "!env | grep -i spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Docker container\n",
    "\n",
    "If you are opening this notebook using the provided container all environment variables are already set.\n",
    "\n",
    "It is the possibile to start the Spark master and workers with the following bash commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$SPARK_HOME/sbin/start-master.sh --host localhost --port 7077 --webui-port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 --cores 2 --memory 2g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the cluster dashboard is available at `localhost:8080`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker cluster\n",
    "\n",
    "If the cluster has been created with Docker compose there is nothing to do. The cluster dashboard can be seen at `localhost:8080` and the master is already available at `spark://spark-master:7077`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Session\n",
    "\n",
    "We can now create the Spark session. \n",
    "\n",
    "With the following command we are asking to the master (and resource manager) to create an _application_ with the required resources and configurations. \n",
    "\n",
    "In this case we are using all the default options, e.g.  number of core and number of executors, but we can also specify them by hand with `.config(\"spark.some.config\", \"value\")`. \n",
    "\n",
    "The list of available configurations can be found [here](https://spark.apache.org/docs/latest/configuration.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# if Spark is run either in Local of Single-Container mode\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    \n",
    "    # build a SparkSession \n",
    "    #   connect to the master node (address `localhost`) and the port where the master node is listening (7077)\n",
    "    #   declare the app name \n",
    "    #   either connect or create a new context\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://localhost:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# if Spark is run as Docker Container cluster (with docker-compose)\n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # build a SparkSession \n",
    "    #   connect to the master node (address `spark-master`) and the port where the master node is listening (7077)\n",
    "    #   declare the app name \n",
    "    #   configure the executor memory to 512 MB\n",
    "    #   either connect or create a new context\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    print(\"Variable CLUSTER_TYPE is not set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the content of the SparkSession `spark` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the SparkContext object from the SparkSession \n",
    "\n",
    "The SparkContext is the main entry point for all Spark functionalities, and it is used to work with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize!\n",
    "\n",
    "The main feauture of Spark is to distribute data in a number of partitions, either using the memory or the disk available in the executors.\n",
    "\n",
    "The first command we will use is `parallelize()`. \n",
    "This function of the `SparkContext` (`sc`) object is used to create a Resilient Distributed Datasets (**RDD**) from a list collection. \n",
    "In other words, it takes a collection and split it amongst the workers. \n",
    "\n",
    "We will start with a basic example to get familiar with the main functions of Spark RDDs. \n",
    "\n",
    "Starting from a python \"dataset\" we create an RDD, `dist_data`, and then operate on it in a parallel fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python dataset\n",
    "data = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "# parallelize\n",
    "dist_data = sc.parallelize(data)\n",
    "\n",
    "# go check the Spark web UI\n",
    "#   http://localhost:8080 for the master node web UI\n",
    "#   http://localhost:4040 for the SparkContext web UI (where you application will reside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "...nothing happend in the Web UI, even though we asked Spark to parallelize our data into a number of partitions! \n",
    "\n",
    "That's because `parallelize` is not an action, but a transformation. \n",
    "We can trigger it by using the `count` action, a function used to count the number of elements in the RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count elements of the rdd\n",
    "dist_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using two cores, in the WebUI you will see two parallel lines for this task. That's because the RDD consists of two *partitions*. Indeed, under the hood, RDD are stored and represented as partitions, \"small blocks\" of the original dataset that will be **unit of parallelism**.\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/rdd_partitions.png'/></center><br />\n",
    "\n",
    "\n",
    "In other words if we have two cores in our worker, but the RDD is composed of one partitions, then only one task at the time will be run. On the other hand, if the partitions are four only two of them will be processed in parallel.\n",
    "\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/partitions_processing.png'/></center><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split our data in an arbitrary number of partitions we can use `numSlices`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize using a different number of partitions\n",
    "sc.parallelize(data, numSlices=8).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(data, numSlices=8).getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the number of partitions\n",
    "dist_data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to visualize that the RDD resides in the worker nodes.\n",
    "If we print the `dist_data` object we will not see the actual data contained in the RDD, but only that this is a distributed dataset.\n",
    "\n",
    "This is actually very important, as in a distributed system the datasets we are processing are typically much larger than the availble memory of the driver (it's one of the main reasons we use a distributed system in the first place).\n",
    "Returning the content of the whole dataset in a single machine is not a clever idea.\n",
    "\n",
    "However, a distributed dataset can be collected in the driver with `collect()`, i.e. all the workers will send back all the blocks of the RDD. \n",
    "\n",
    "This is generally used to collect the end-results, after a computationally heavy processing is first run on the worker nodes. \n",
    "This function should be used with caution since it fetches the entire RDD to a single machine and can cause the driver to run out of memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the data_rdd\n",
    "dist_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map and Reduce\n",
    "\n",
    "We can now write our first map-reduce application. \n",
    "\n",
    "The method `map(f)` will apply the function `f` to each _element_ of the RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increment each number of the array with map\n",
    "# and collect the result\n",
    "dist_data.map(  \"\"\" increment by 1 \"\"\"  ).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from plain Hadoop, in Spark we chain together multiple transformations, without having to strictly follow the Map-then-Reduce pattern.\n",
    "\n",
    "For instance, we can even run an entire chain of map functions one after the other with no reduce function at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple map transformations\n",
    "# plus collect (action)\n",
    "dist_data.map(\"\"\" increment by 1 \"\"\")\\\n",
    "         .map(\"\"\" divide by 2    \"\"\")\\\n",
    "         .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduce` is an action that aggregates all the elements of the RDD using some function and returns the result to the driver. \n",
    "\n",
    "We will see later on a version of reduce, `reduceByKey`, wich performs the reduction for elements with the same key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce actions can take a function as argument, which will be applied to the elements of the RDD, to perform some kind of aggregation.\n",
    "\n",
    "For example, we can think about summing the elements of a RDD in pairs with `reduce(lambda x, y: x + y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increment each number of the dataset (transformation)\n",
    "# and sum all of them in pairs (action)\n",
    "dist_data.map(\"\"\" increment by 1 \"\"\").reduce(\"\"\" sum all values in pairs \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schematic view of this simple map-reduce application can be seen here:\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/map_reduce_increment.png'/></center><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1\n",
    "\n",
    "Write a simple reduce function to find the minimum of `dist_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - compute $\\pi$\n",
    "\n",
    "It is possible to estimate $\\pi$ by simulating random points in the unit square (side length of $1$) and counting how many fall in the unit circle. The probability of one point falling inside the circle is \n",
    "\n",
    "$$\n",
    "P = \\frac{\\text{Area circle}}{\\text{Area square}} = \\frac{\\pi}{4}\n",
    "$$\n",
    "\n",
    "We can estimate this probabiliy by counting the number of simulated points inside the circle\n",
    "\n",
    "$$\n",
    "P = \\frac{\\text{#Points in circle}}{\\text{#Points}} \n",
    "$$\n",
    "\n",
    "Thus obtaining \n",
    "\n",
    "$$\n",
    "\\pi \\approx 4 \\cdot \\frac{\\text{#Points in circle}}{\\text{#Points}}\n",
    "$$\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/pi_estimation.png'/></center><br/>\n",
    "\n",
    "This can be done in Spark running the computation in parallel with the following steps:\n",
    "1. Create the \"dummy RDD\" containing placeholders for the points (e.g. use the value 0 as placeholder for each data point)\n",
    "2. Generate the points as random (x,y) pairs and check if each of them falls inside/outside the circle\n",
    "3. Count the points inside the circle\n",
    "\n",
    "Think about which transformations are needed to generate the points and to check if they fall inside the circle.\n",
    "Combine all the transformations and the action in a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_points = \n",
    "\n",
    "## instantiate \"placeholder RDD\"\n",
    "points_rdd = \n",
    "\n",
    "def in_circle(dummy):\n",
    "    ## simulate the point and check if\n",
    "    ## it is inside the circle \n",
    "    ## return 0 or 1\n",
    "\n",
    "# count points inside the circle\n",
    "\n",
    "# print result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result can be achieved using the `filter` transformation. \n",
    "\n",
    "`filter(f)` returns a new RDD containing only the element of source RDD on which `f` is `true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the same exercise using filter\n",
    "count_circle = points_rdd \\\n",
    "    .map(\"\"\" the map function \"\"\") \\\n",
    "    .filter(\"\"\" the filter function \"\"\") \\\n",
    "    .count()\n",
    "\n",
    "# print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce by key and flat map\n",
    "\n",
    "Consider the following dataset where each element consists of a tuple `(group, value)` (a **key-value pair**). \n",
    "\n",
    "This can be for example:\n",
    "- `group` = product class, and \n",
    "- `value` = revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = [('group1', 10), ('group2', 4), ('group3', 1), ('group2', 7), ('group1', 8)]\n",
    "class_rdd = sc.parallelize(class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could be interested in operating only on the values of the dataset, discarding the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operate only on the values\n",
    "class_rdd.map(lambda el: (el[0], el[1]+1)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result can be achieved using `mapValues`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same using map values\n",
    "class_rdd.mapValues(\"\"\" increment values by 1 \"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a reduce function for each class using `reduceByKey`: in this way we are applying the reduce function only to the elements of the same class.\n",
    "\n",
    "Despite its name, `reduceByKey` is not an action, but a transformation, since it returns a distributed dataset (the result of an aggregation by key could still be very large if the number of keys is large).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the minimum using reduce by key\n",
    "class_rdd.reduceByKey(\"\"\" find the minimum per group \"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further filter our results by using `takeOrdered` to get the first 2 results ordered by key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_rdd.reduceByKey(\"\"\" find the minimum per group \"\"\") \\\n",
    "    .takeOrdered(2, key=lambda x: \"\"\" sort by the value in descending order \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are not interested in treating the dataset as key-value pairs, or in general we have a non-flat dataset (e.g. each element of the RDD is a list, or another object), we can use `flatMap` to \"explode\" each element returning a plain sequence of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the rdd\n",
    "class_rdd.flatMap(\"\"\" flatten all elements of the rdd \"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([[1,2,3], [2,3,4]]) \\\n",
    "    .flatMap(lambda x: x) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: word count\n",
    "\n",
    "You have received the following message from one of your friend: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    'One ring to rule them all, ? one ring to find them,\\n',\n",
    "    'One ring $ to bring them all, and in the % darkness bind them;\\n',\n",
    "    'In the Land of Mordor @ where the shadows lie.'\n",
    "]\n",
    "\n",
    "print(''.join(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are interested in counting the occurence of each word.\n",
    "\n",
    "First, you may want to need to clean-up the message (e.g remove symbols such as `@` and `$`).\n",
    "\n",
    "You also want to change all the words to lower case, to avoid counting the same word twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize the message\n",
    "message_rdd = sc.parallelize(message)\n",
    "message_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: use `string.punctuation` or a regular expression to remove the unwanted character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: create the words_rdd where each element is a word\n",
    "# eg: ['one', 'ring', 'to', 'rule', 'them']\n",
    "\n",
    "def clean_row(row):\n",
    "    # clean the row and return the list of words\n",
    "    word_list = []\n",
    "    \n",
    "    ## Code\n",
    "\n",
    "    ##\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "# apply transformation\n",
    "words_rdd = \n",
    "words_rdd.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: perform word count!\n",
    "# -> crate pairs (word, 1)\n",
    "# -> reduce and count\n",
    "\n",
    "word_count_rdd = \n",
    "word_count_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `takeOrdered(n, key)` allows to collect only the first $n$ elements based on the ordering provided by key. Ordering is by default ascending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the 3 most occurent words\n",
    "word_count_rdd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Linear regression\n",
    "\n",
    "The files in `../datasets/lecture1/` contain the measure of a sensor. Each measure is in the form \n",
    "\n",
    "```\n",
    "Measure  N: (t,val)\n",
    "```\n",
    "\n",
    "There could be pairs such as `(t, None)`, corresponting to missing values. This measure should be removed. \n",
    "\n",
    "We are interested in knowing if there is any sort of relation between `t` and `val`.\n",
    "\n",
    "The first task consits in reading each file and creating an RDD containing as element the pairs `(t,val)`.\n",
    "\n",
    "We can first inspect the first lines of a file to get a sense of the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../datasets/lecture1/file_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that each file contain millions of measure and there may be hundreds of files. Moreover, data can be already saved in files in multiple nodes. \n",
    "\n",
    "For the purpose of this exercise, we will assume that the data is already distributed in the cluster.\n",
    "\n",
    "For this reasons reading all the files with python, and then using `parallelize` should be avoided at all costs.\n",
    "This would be a very inefficient way to read the data, as we would have to read all the files in the driver (possibly getting out of memory), then splitting the data in the workers. \n",
    "\n",
    "Instead, we can parallelize the _list of files_ and then read them in parallel with Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLUSTER_TYPE=='local':\n",
    "    base_path = '/home/pazzini/work/courses/MAPD_B/MAPD-B/spark/datasets/lecture1/file_{}.txt'\n",
    "else:\n",
    "    base_path = '/opt/workspace/datasets/lecture1/file_{}.txt'\n",
    "file_list = [base_path.format(i) for i in range(1,5)]\n",
    "\n",
    "files_rdd = sc.parallelize(file_list)\n",
    "files_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "In this case we could have used the function `textFile`, which takes as input the path to the files and read them into an RDD. \n",
    "\n",
    "To access a file in with this function we need to prepend `file://` if the file is in the local file system. If the file is in hadoop the path will be similar to `hdfs://namenode.com:8020/path_to_the_file`. In the same way, if the file is stored in `s3` the path will be `s3://`.\n",
    "\n",
    "The function `textFile` can be used in this case only because files are stored in `.txt` files.\n",
    "In many cases, however, a custom data loader is required instead.\n",
    "On the other hand, we should try to use this function as much as possible since they are much more performant than standard python code.  \n",
    "\n",
    "```python\n",
    "text_rdd = sc.textFile(\"file://\"+base_path.format('*'))\n",
    "text_rdd.collect()[:4]\n",
    "```\n",
    "\n",
    "However, in this example we will wire a custom data loader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the RDD files, write a map that convert it into an RDD, `data_rdd`, where each element is a tuple `(t, val)`. \n",
    "\n",
    "Remember to remove points with `None` as measure. \n",
    "\n",
    "The elements of `data_rdd` should finally be something like:\n",
    "\n",
    "```\n",
    "[(0.0,9.93), (-1.0,9.02), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this, we can use _regular expressions (or regex)_.\n",
    "\n",
    "A regular expression is a sequence of characters that allows to identify and select a **pattern** in a string, instead of a specific substring.\n",
    "Most editors (including Jupyter) allow to use regex to find and replace text based on patterns, and several interesing sites offer a simple interface to learn and experiment with regexp, e.g. https://regex101.com/\n",
    "\n",
    "All following charachters are considered \"meta-characters\" in regex\n",
    "\n",
    "`. ^ $ * + ? { } [ ] \\ | ( )`\n",
    "\n",
    "In a regular expression, all these symbols will be **not** interpreted as their characters, but are used to build up the patterns and search for matches in the text.\n",
    "\n",
    "The main notable examples are:\n",
    "\n",
    "`[ ]` square brakets meaning exactly 1 character\n",
    "\n",
    "| regexp | interpretation |\n",
    "| --- | --- |\n",
    "| \\[a\\] | the letter `a` |\n",
    "| \\[ax\\] | the letter `a` or `x` |\n",
    "| \\[a-x\\] | any letter from `a` to `x` |\n",
    "| \\[a-zA-F\\] | any letter from `a` to `z` and from `A` to `F` |\n",
    "| \\[0-9\\] | any number from 0 to 9 |\n",
    "\n",
    "`^` represents a `not`\n",
    "\n",
    "| regexp | interpretation |\n",
    "| --- | --- |\n",
    "| \\[^b02\\] | all charachters but `b`,`0`and `2` |\n",
    "\n",
    "`.` represents any character at all\n",
    "\n",
    "| regexp | interpretation |\n",
    "| --- | --- |\n",
    "| . | all charachters |\n",
    "\n",
    "`?` makes the preceding character in the regular expression optional\n",
    "\n",
    "| regexp | interpretation |\n",
    "| --- | --- |\n",
    "| colou?r | `color` or `colour` |\n",
    "\n",
    "`*` match the preceding character (or combination) zero or more times\n",
    "\n",
    "| regexp | interpretation |\n",
    "| --- | --- |\n",
    "| go*gle | `ggle`,`gogle`,`google`,`gooogle`,... |\n",
    "\n",
    "`\\t` matches a tab\n",
    "\n",
    "`\\n` matches a new-line\n",
    "\n",
    "`^` (outside of `[]`) matches the start of a line\n",
    "\n",
    "`$` matches the end of a line\n",
    "\n",
    "Regex can be used in plain Python with the `re` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `re` module to implement the appropriate regular expression to clean up our data.\n",
    "\n",
    "We should find all float values possibly starting with a +/- sign and collecting them in pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "line_ = 'Measure  0: (.05,0.70)'\n",
    "float_pattern = '-?[0-9]*\\.[0-9]*'\n",
    "coordinates = re.findall(f'{float_pattern},{float_pattern}', line_)\n",
    "coordinates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_file(file):\n",
    "    \n",
    "    ## load lines \n",
    "    with open(file, 'r') ...\n",
    "        \n",
    "    points = []\n",
    "    ## extract points\n",
    "    for line in lines ... \n",
    "    \n",
    "    ## \n",
    "    return points\n",
    "\n",
    "data_rdd = \n",
    "data_rdd.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `sample(with_replacement, fraction)` we can sample some points from the original RDD. \n",
    "\n",
    "This is useful if we want to collect only a fraction of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "# collect a sample\n",
    "data = np.array(data_rdd.sample(False,0.2).collect())\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "\n",
    "plt.xlabel(r'$p_x$')\n",
    "plt.ylabel(r'$p_y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a distributed gradient descent and use it to find the best parameters for a linear model. \n",
    "\n",
    "Given an input $X$ the ouput of the model is \n",
    "\n",
    "$$\n",
    "Y = W X = w_{0} + w_1 x_1 + \\dots + w_p x_p\n",
    "$$\n",
    "\n",
    "In our case, we have $Y=y(w,x)$, $W = [w_0, w_1]$ and $X=[1, x]^T$, in other words\n",
    "\n",
    "$$\n",
    "y(w,x) = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "We are interested in estimating the optimal parameters $w^\\star$ of the line fitting our data. \n",
    "\n",
    "To do this we will use **gradient descent**, an iterative procedure that allows us to find a local minumum of a differentiable function.\n",
    "\n",
    "In our case, we would like to minimize the square residuals, i.e.\n",
    "\n",
    "$$\n",
    "J(W,X) = \\frac{1}{2n} \\sum_{i=1}^{n} [Y(W, X)- y_i]^2 = \\frac{1}{2n} \\sum_{i=1}^{n} [ (w_0 +w_1x) - y_i]^2\n",
    "$$\n",
    "\n",
    "where $y_i$ is the true value. \n",
    "\n",
    "In each step of the gradient descent we use the following update rule\n",
    "\n",
    "$$\n",
    "W_{i+1} = W_i - \\gamma \\nabla J_W(W_i, X)\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is the learning rate, a variable used to reduce the size of each step.\n",
    "\n",
    "\n",
    "In other word we are moving in the opposite direction of the gradient, i.e. towards the minimum of the function. \n",
    "\n",
    "Recalling that $W=[w_0, w_1]$ and $X=[1,x]$ we have that each component of the gradient, i.e. $\\frac{\\partial}{\\partial w_p} J(W,X) $\n",
    "\n",
    "$$\n",
    "\\nabla J(W, X) = \\left[\\frac{1}{n} \\sum_{i=1}^{n} [Y(W,X)- y_i]\\cdot1, \\frac{1}{n} \\sum_{i=1}^{n} [Y(W,X)- y_i]\\cdot x_i \\right]\n",
    "$$\n",
    "\n",
    "We can now write a map-reduce job used to estimate the parameters using gradient descent on the full dataset.\n",
    "\n",
    "We start by defining the weights vector and initializing it to some values, e.g. $(10, 0.5)$ is a good guess :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy to create the W weight array\n",
    "W = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement then the functions computing the prediction given as input $x$ and the current weights W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict y for any given x and weights W\n",
    "def predict(x, W):\n",
    "    # return prediction    \n",
    "\n",
    "# test the function in local \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function computing the gradient for one example, given as inputs:\n",
    "1. the point $P=(x,y)$, and \n",
    "2. the current set of weights $W$. \n",
    "\n",
    "Remember that the gradient has two components, one per parameter. \n",
    "\n",
    "Furthermore, the normalization $\\frac{1}{n}$ can be ommited since we can apply it afterwards, having summed the gradients of all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradient \n",
    "def gradient(P, W):\n",
    "    # return the prediction given the x value of P and the weights W\n",
    "    \n",
    "    # compute gradient\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# test the function in local\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implement the gradient descent and find the optimal line parameters. \n",
    "\n",
    "**Hint**: compute the gradient for each point in parallel, and them sum all of them. Finally, use this sum to update the weights vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count points based on the partitioned data\n",
    "num_points = \n",
    "\n",
    "# re-declare the weight vector here: \n",
    "#   this is useful if the cell is run multiple times\n",
    "W = \n",
    "\n",
    "# define the learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# define the number of iterations\n",
    "num_it = 20\n",
    "\n",
    "for i in range(num_it):\n",
    "    # run the gradient descent in parallel and then sum the gradients \n",
    "    \n",
    "    # update the weights according to the learning rate and the gradient\n",
    "    \n",
    "    \n",
    "print(\"Final parameters: x0={:.2f}, x1={:.2f}\".format(W[0], W[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the results by plotting the data points and the resulting best fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.array(data_rdd.collect())\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "\n",
    "x = np.arange(-10,11)\n",
    "y = W[0] + W[1]*x\n",
    "\n",
    "plt.plot(x, y, color='red', lw=2)\n",
    "\n",
    "plt.xlabel(r'$x_{0}$')\n",
    "plt.ylabel(r'$x_{1}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "From the WebUI we can see that in each iteration Spark is computing every operation from the very beginning, i.e. **the parallelization of the text files!**. \n",
    "This is not really necessary, as this first operation will not need to be re-executed every step.\n",
    "\n",
    "With `persist()` we can tell spark to cache the intermediate results, e.g. after the function parsing the files, into the executors' memory. \n",
    "\n",
    "In this way the same dataset will be loaded in the next iterations much faster, at the cost of having the dataset stored in memory. \n",
    "\n",
    "**Note:** to be precise, there could be different levels of [persistence](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence) of data into the executors' memory.\n",
    "\n",
    "The RDD can be \"unpersisted\" with `unpersist()`, freeing up memory when needed.\n",
    "\n",
    "Performe the gradient descent iterations again by caching `data_rdd` ad look at the WebUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# persist the original RDD\n",
    "data_rdd = data_rdd.persist()\n",
    "\n",
    "# --- same code as previously ---\n",
    "\n",
    "\n",
    "\n",
    "print(\"Final parameters: x0={:.2f}, x1={:.2f}\".format(W[0], W[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up the memory\n",
    "data_rdd = data_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals\n",
    "\n",
    "Compute the residuals using Spark, and plot it in a histogram. \n",
    "\n",
    "The residual of the point $(x_i, y_i)$ with respect to the model $y(x)$ is simply defined as\n",
    "\n",
    "$$\n",
    "R_i = y(x_i) - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_rdd = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals_rdd.collect(), bins=25)\n",
    "plt.xlabel(\"residuals\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop worker and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the running Spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the running Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark services - Local cluster\n",
    "\n",
    "To stop the Spark execution in the master and worker nodes, issue the following bash commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$SPARK_HOME/sbin/stop-worker.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$SPARK_HOME/sbin/stop-master.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark services - Docker container(s)\n",
    "\n",
    "For the single Docker container, it is sufficient to stop and close the running container.\n",
    "\n",
    "For the Docker cluster, use `docker-compose down` to stop and clear all running containers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
