{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37dfaa9",
   "metadata": {},
   "source": [
    "# Kafka Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d54a37",
   "metadata": {},
   "source": [
    "## Interacting with Kafka Producer from the shell\n",
    "\n",
    "The Producer is the Kafka abstraction for publishing data _into_ a Kafka topic.\n",
    "\n",
    "It is completely detached from any possible Consumer process; therefore, there is no need to directly connect a Producer (Sender) and a Consumer (Receiver) in Kafka due to its PUB/SUB messaging model.\n",
    "\n",
    "However, in order for a Producer to publish a message, we need to specify at least two things:\n",
    "1. The topic to which the messages will be published.\n",
    "2. The location of the cluster over the network.\n",
    "\n",
    "Clearly, the goal is to work with Kafka programmatically (from Python/C/...) to interface it with other possible applications.\n",
    "However, for a first glance, we will interact with the Kafka cluster in _interactive mode_ from the shell.\n",
    "\n",
    "Apache Kafka provides a set of bash scripts to interact with and operate the cluster for basic operations and testing, such as:\n",
    "- Topic creation, configuration, and inspection.\n",
    "- Shell-based message producer.\n",
    "- Shell-based message consumer.\n",
    "- Shell-based performance testing.\n",
    "- ...\n",
    "\n",
    "Let's first connect to the Kafka cluster from the shell by logging into the Kafka-Broker container:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161de49",
   "metadata": {},
   "source": [
    "`$ docker exec -it <your_kafka-broker_container_name> bash`\n",
    "\n",
    "e.g.:\n",
    "\n",
    "`$ docker exec -it kafka-kafka-broker-1 bash`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a947f7c",
   "metadata": {},
   "source": [
    "The Kafka installation is located in the folder ``. \n",
    "Inside that folder, you can find the binaries (the \"executables\") to use Kafka directly.\n",
    "\n",
    "To navigate to the `bin` folder, use the following command:\n",
    "`cd $KAFKA_HOME/bin`\n",
    "\n",
    "From there, you can use the basic Kafka binaries from the shell to issue commands such as:\n",
    "\n",
    "- List all available topics:\n",
    "\n",
    "`./kafka-topics.sh --list --bootstrap-server kafka-broker:9092`\n",
    "\n",
    "- Create a new topic:\n",
    "\n",
    "`./kafka-topics.sh --create --topic my_awesome_topic --bootstrap-server kafka-broker:9092`\n",
    "\n",
    "- Describe a specific topic:\n",
    "\n",
    "`./kafka-topics.sh --describe --topic my_awesome_topic --bootstrap-server kafka-broker:9092`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1384cc",
   "metadata": {},
   "source": [
    "We can also use the shell to send messages to the newly created topic:\n",
    "\n",
    "To start the Kafka console producer, use the following command:\n",
    "\n",
    "`./kafka-console-producer.sh --topic my_awesome_topic --bootstrap-server kafka-broker:9092`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443750b0",
   "metadata": {},
   "source": [
    "At this point, you should be able to send messages to the topic you just created via the `kafka-console-producer`.\n",
    "\n",
    "So far, no consumer is available to process or display those messages. However, the messages are successfully sent to the topic, increasing the log(s) in the (possibly more than one) partition(s).\n",
    "\n",
    "Let's create a console consumer and subscribe to the topic.\n",
    "\n",
    "To do this, connect to the same Docker container running the Kafka broker and navigate to the folder containing the Kafka binaries.\n",
    "\n",
    "From there, you can:\n",
    "\n",
    "- Consume the messages from our previously created topic:\n",
    "  \n",
    "`./kafka-console-consumer.sh --topic my_awesome_topic --bootstrap-server kafka-broker:9092 [--from-beginning]`\n",
    "\n",
    "**Note:** We can instruct the *Consumer* to go back and read data from the beginning, but the *Producer* can only append new data to the log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f1dab",
   "metadata": {},
   "source": [
    "## Programmatically produce messages to Kafka with Python\n",
    "\n",
    "There are various Python modules available to interact with Kafka programmatically, including:\n",
    "- kafka-python\n",
    "- confluent-kafka\n",
    "- pyKafka\n",
    "\n",
    "The differences between these modules are relatively minor. As always, take your time to glance at the documentation of all alternatives before starting a project.\n",
    "\n",
    "For now, we'll use `kafka-python` to handle topics and producers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ef456",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kafka-python # confluent-kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a7968",
   "metadata": {},
   "source": [
    "To instantiate a Kafka producer using `kafka-python`, you can use the following code:\n",
    "\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# Create a Kafka producer instance\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['62.30.10.23:9092'],  # List of brokers\n",
    "    security_protocol=\"SSL\",                 # Security protocol (if any) \n",
    "    ssl_cafile=\"./ca.pem\",                   # Certificate details (if any)\n",
    "    ssl_certfile=\"./service.cert\",           # ...\n",
    "    ssl_keyfile=\"./service.key\",             # ...\n",
    "    value_serializer=msgpack.dumps           # Message value serialization function \n",
    "                                             # (e.g., interpreting the message as a specific format)\n",
    ")\n",
    "```\n",
    "\n",
    "We'll work with the vanilla version of the producer, which does not require any certificates or specific serialization in this example.\n",
    "\n",
    "Here's an example of a simple producer instantiated by pointing it to the Kafka brokers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc42696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the list of brokers in the cluster\n",
    "KAFKA_BOOTSTRAP_SERVERS = ['kafka-broker:9092']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "# Create a Kafka producer instance\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276dbb1",
   "metadata": {},
   "source": [
    "Let's publish a message to the topic we previously created _without specifying any key_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63dc02c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# publish a message to a topic without a key\n",
    "producer.send(topic='my_awesome_topic', \n",
    "              value=b'message 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d92ec3",
   "metadata": {},
   "source": [
    "The output message `<kafka.producer.future...>` explicitly tells us that the record has been created and will be sent at some point.\n",
    "\n",
    "However, when we pressed return on the previous cell, the message had not been sent yet.\n",
    "\n",
    "The `KafkaProducer.send()` method is _asynchronous_, which means it enqueues the message on an internal queue. The actual sending of the message to the broker happens later, based on tunable parameters like the maximum buffering time or the number of messages.\n",
    "\n",
    "This behavior is beneficial because packing multiple messages in small batches improves data transfer efficiency. While sending a single message may not show noticeable performance differences, imagine sending billions of messages per day, which translates to millions of messages per second. Optimizing data transfer and minimizing communication overhead between producers and topics becomes crucial.\n",
    "\n",
    "It's important to be aware that messages won't necessarily be sent immediately. If a high message rate is sent and the `exit()` command is issued right after a `producer.send()` call, it's possible that no messages are actually sent because the maximum buffering time or the number of messages has not been reached before the program exits.\n",
    "\n",
    "To send messages synchronously, you can use the `flush()` method of the producer. It ensures that all outstanding messages are sent before proceeding.\n",
    "\n",
    "For more information on the tunable parameters and additional functionalities, you can refer to the KafkaProducer API documentation: [link to KafkaProducer API](https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55004ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish a message and flush the queue right away\n",
    "producer.send('my_awesome_topic', b'a new message')\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a795d18f",
   "metadata": {},
   "source": [
    "It's important to realize that producers and consumers are completely decoupled in Kafka. This means that the functioning of a consumer is not affected even if a producer \"dies\".\n",
    "\n",
    "The decoupled nature of Kafka allows consumers to access the topic and consume messages regardless of the state of the producer.\n",
    "\n",
    "Even if a producer dies or experiences an issue, the messages previously sent and stored on the brokers are still accessible to consumers. This ensures data durability and enables fault tolerance in the Kafka ecosystem. Consumers can continue to consume messages from the topic without any interruption, as long as the messages are available on the brokers.\n",
    "\n",
    "This decoupling of producers and consumers in Kafka is a key architectural feature that enables scalability, fault tolerance, and flexibility in building distributed systems.\n",
    "\n",
    "It's worth noting that while consumers are not directly impacted by the state of the producer, they may experience delays in receiving messages if there are any disruptions in the producer's ability to send messages. However, once the producer resumes normal operation, the consumers can continue consuming messages from where they left off.\n",
    "\n",
    "This decoupled nature of Kafka makes it well-suited for building robust and scalable distributed systems where producers and consumers can operate independently and asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060bd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the producer\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Kafka producer and send a new message to the same topic\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "producer.send('my_awesome_topic', b'a message from the revived producer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64219d71",
   "metadata": {},
   "source": [
    "## Messages with Key\n",
    "\n",
    "In Kafka, messages are structured as `<key, value>` pairs.\n",
    "\n",
    "So far, we have produced messages only with a given `value`, but it's also possible to include a `key` for each message. The `key` serves as an optional identifier for the message and can be used for various purposes, such as routing messages to specific partitions within a topic.\n",
    "\n",
    "When a producer sends a message with a key, Kafka uses the key to determine the partition to which the message should be assigned. By default, Kafka uses a _hashing algorithm_ to evenly distribute messages across partitions based on their keys. \n",
    "This ensures that messages with the same key are always assigned to the same partition, providing a form of message ordering within a partition.\n",
    "\n",
    "Adding a key to messages can be useful in scenarios where you want to ensure ordering or group related messages together based on a common key. It enables consumers to process messages in a consistent and predictable manner.\n",
    "\n",
    "To include a key when producing messages, you can modify the previous code example as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send messages to Kafka using a specific key\n",
    "producer.send(topic='my_awesome_topic', \n",
    "              key=b'some_key', \n",
    "              value=b'a message with key')\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e702b6",
   "metadata": {},
   "source": [
    "## Create a topic from kafka-python\n",
    "\n",
    "Using the kafka-python library, you can administer a Kafka cluster and create new topics with specific configuration parameters, such as the replication factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87899de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages to administer the Kafka cluster\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "# connect to the cluster to run admin functions\n",
    "kafka_admin = KafkaAdminClient(\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb1a9a",
   "metadata": {},
   "source": [
    "It is possible to retrieve the list of topics, equivalent of issuing the following using the Kafka binaries from shell:\n",
    "\n",
    "`./kafka-topics.sh --list --bootstrap-server kafka-broker:9092`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc007dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the available topics\n",
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74759c50",
   "metadata": {},
   "source": [
    "Topics are partitioned entities.\n",
    "Within each partition events are added to the end of the log, resulting in an ordered list of records.\n",
    "\n",
    "Publishing a new message to a partitioned topic will result in the addition of the message to the end of the log retained on the owner of a specific partition. If replication is enabled, the message will be then ridistributed to the other follower partitions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Topics in Kafka are partitioned entities. Each topic is divided into multiple partitions, and within each partition, events are added to the end of a log, resulting in an ordered list of records.\n",
    "\n",
    "When you publish a new message to a partitioned topic, the message is appended to the end of the log retained by the owner of the specific partition to which the message is assigned. \n",
    "\n",
    "Each partition has one leader and zero or more follower replicas. If replication is enabled for the topic, the message will be replicated and distributed to the follower replicas of the partition.\n",
    "\n",
    "The leader partition handles all read and write operations for the partition, while the follower replicas replicate the log from the leader to ensure fault tolerance and high availability. If the leader partition fails, one of the follower replicas will be elected as the new leader to continue processing messages for that partition.\n",
    "\n",
    "The use of partitions provides several benefits in Kafka including Scalability and Parallelism\n",
    "\n",
    "It's important to note that the distribution and replication of messages across partitions are managed internally by Kafka and are transparent to the producer and consumer applications. The producer only needs to specify the topic and optionally the key for a message, and Kafka takes care of routing the message to the appropriate partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc69a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new topic with the following parameters:\n",
    "#    number of partitions = 2\n",
    "#    replication factor   = 1 (i.e. no replication)\n",
    "a_new_topic = NewTopic(name='a_partitioned_topic', \n",
    "                       num_partitions=2, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[a_new_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the available topics\n",
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc08a9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Publish messages for the Spark Structured Streaming example\n",
    "\n",
    "Kafka can be used as a source for incoming messages in Spark Streaming and Structured Streaming.\n",
    "\n",
    "We'll use the pySpark `Structured Streaming` API to implement the example previously seen in the Spark hands-on sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8306a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "first_names = ('John', 'Andy', 'Joe', 'Alice')\n",
    "last_names = ('Johnson', 'Smith', 'Jones', 'Millers')\n",
    "\n",
    "\n",
    "# while 1: # Uncomment this to send a continuous stream of messages\n",
    "for i in range(20):\n",
    "    msg = {\n",
    "        'name': random.choice(first_names),                    # Select a random first name\n",
    "        'surname': random.choice(last_names),                  # Select a random last name\n",
    "        'amount': '{:.2f}'.format(random.random() * 1000),     # Generate a random amount\n",
    "        'delta_t': '{:.2f}'.format(random.random() * 10),      # Generate a random delta_t\n",
    "        'flag': random.choices([0, 1], weights=[0.8, 0.2])[0]  # Randomly choose a flag value\n",
    "    }\n",
    "    producer.send('a_partitioned_topic',\n",
    "                  json.dumps(msg).encode('utf-8')  # Convert the message to JSON and encode as UTF-8\n",
    "                  )\n",
    "    producer.flush()  # Flush the producer buffer\n",
    "    time.sleep(0.25)  # Sleep for a short duration before sending the next message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076f3da",
   "metadata": {},
   "source": [
    "Let's create a new topic to store the `results` of the Spark processing of the Kafka stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a87c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new topic\n",
    "a_new_topic = NewTopic(name='results', \n",
    "                       num_partitions=2, \n",
    "                       replication_factor=1)\n",
    "\n",
    "# create the new topic\n",
    "kafka_admin.create_topics(new_topics=[a_new_topic])\n",
    "\n",
    "# check the list of available topics\n",
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd152462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
